//============================================================================
// Name        : CppAlgorithms.cpp
// Author      : Anderson Paschoalon
// Version     :
// Copyright   : Your copyright notice
// Description : Hello World in C++, Ansi-style
//============================================================================
#define TEST_FUNCTIONS

#include <stdio.h>
#include <stdlib.h>
#include <malloc.h>
#include <iostream>
#include <fstream>
#include <list>
#include <vector>
#include <array>
#include <cstdlib>
#include <cmath>
#include <armadillo>

using namespace std;
using namespace arma;

typedef struct stochastic_model_fitting
{
	double aic;
	double bic;
	string modelName;
	double param1;
	double param2;
	int size;
} StochasticModelFit;

int compareBic(const void* a, const void* b)
{

	if ((*(StochasticModelFit*) a).bic < (*(StochasticModelFit*) b).bic)
		return -1;
	if ((*(StochasticModelFit*) a).bic == (*(StochasticModelFit*) b).bic)
		return 0;
	if ((*(StochasticModelFit*) a).bic > (*(StochasticModelFit*) b).bic)
		return 1;

	return (0);
}

int compareAic(const void* a, const void* b)
{

	if ((*(StochasticModelFit*) a).aic < (*(StochasticModelFit*) b).aic)
		return -1;
	if ((*(StochasticModelFit*) a).aic == (*(StochasticModelFit*) b).aic)
		return 0;
	if ((*(StochasticModelFit*) a).aic > (*(StochasticModelFit*) b).aic)
		return 1;

	return (0);
}

class DataProcessor
{

public:

	/**
	 * 	ACTUAL FUNCTIONS
	 */
	// Take as input a vector <T> vet[], its first and last position to be
	// sorted. After the execution vet[] will be sorted.
	// T* vet : C vector
	// int left: fist position of the C vector to be sorted, usually 0.
	// int right: last position of the C vector to be sorted, usually size-1;
	template<typename T> void quickSort(T* vet, int left, int right);
	// Evaluate the mode (the most frequent value) of a list list<T>
	template<typename T> T mode(list<T>* theList);
	// convert a list to a c vector
	template<typename T> T* list_to_cvector(list<T>* theList);
	template<typename T> void delete_cvector(T* c_vet);
	//compare two vectors
	template<typename T> bool isEqual(const T* vet1, const T* vet2,
			const int size);
	template<typename T> void vectorC_to_list(list<T>* theList, T* vet,
			int size);

	/**
	 * Return an empirical vector of the Cumulative distribution function.
	 * This method take as input a list<double> of empirical data, and create a
	 * vector (of the same size) of an empirical evaluation of the cumulative
	 * distribution function.
	 * @param list<double>
	 * @return vec*
	 */
	vec* empiricalCdf(list<double>& empiricalData);

	//feature matrix: return a matrix X(m, 2) = [1 data]. Must be freed using delete.
	mat* featureMatrix(list<double>& empiricalData);
	mat* featureMatrix(const vec& empiricalData);
	//compute cost for gradient descendent
	double computeCost(const mat& X, const vec& y, const vec& theta);
	// gradient descendent algorithm
	void gradientDescendent(const mat& X, const vec& y,
			const double learning_rate, const int num_iters, vec& theta,
			vec& J_history);
	//information criterion
	double informationCriterion(const vec& data, const string& functionName,
			const vec& paramVet, const string& criterion);
	//natural logarithm of likehood function
	double logLikehood(const vec& data, const string& functionName,
			const vec& paramVet);
	// fit weibull alpha and betha using linear regression
	void weibullFitting(const vec& interArrival, const vec& interArrivalCdf,
			vec& paramVec, vec& informationCriterion);
	// evaluate mean and standard deviation
	void normalFitting(const vec& interArrival, vec& paramVec,
			vec& informationCriterion);
	// evaluate lambda using linear regression
	void exponentialLrFitting(const vec& interArrival,
			const vec& interArrivalCdf, vec& paramVec,
			vec& informationCriterion);
	// evaluate lambda using mean estimation
	void exponentialMeFitting(const vec& interArrival, vec& paramVec,
			vec& informationCriterion);
	// evaluate pareto's alpha and xm using Linear regression
	void paretoLrFitting(const vec& interArrival, const vec& interArrivalCdf,
			vec& paramVec, vec& informationCriterion);
	// evaluate pareto's alpha and xm using maximum likehood method
	void paretoMlhFitting(const vec& interArrival, const vec& interArrivalCdf,
			vec& paramVec, vec& informationCriterion);
	// evaluate cauchy's gamma and x0 using linear regression
	void cauchyFitting(const vec& interArrival, const vec& interArrivalCdf,
			vec& paramVec, vec& informationCriterion);
	void constantFitting(const vec& interArrival, vec& paramVec,
			vec& informationCriterion);


	//stochastic functions
	double pdf_weibull(double x, double alpha, double betha);
	double cdf_weibull(double x, double alpha, double betha);
	double pdf_exponential(double x, double lambda);
	double cdf_exponential(double x, double lambda);
	double pdf_pareto(double x, double alpha, double xm);
	double cdf_pareto(double x, double alpha, double xm);
	double pdf_cauchy(double x, double gamma, double x0);
	double cdf_cauchy(double x, double gamma, double x0);
	double pdf_normal(double x, double mu, double sigma);
	double cdf_normal(double x, double mu, double sigma);
	double pdf_uniform(double x_min, double x_max);
	double cdf_uniform(double x, double x_min, double x_max);

	// Take as input a list o inter-arrival times, and a pointer to a C vector
	// of the struct StochasticModelFit. The available models are ordered here,
	// from the best to the worst, according to the specified criterion BIC or
	// AIC. If it is not specified, it uses as default AIC.
	StochasticModelFit* fitModels(list<double>& empiricalData,
			const string& criterion);

#ifdef TEST_FUNCTIONS

	void save_data_on_file(const string& fileName, const mat& vet1,
			const mat& vet2);
	void printTestResult(string testName, bool result);
	bool compareDouble(double val1, double val2);
	bool compareDouble(double val1, double val2, double acErr);
	bool compareMat(mat& mat1, mat& mat2);
	bool testQuickSort();
	bool testMode();
	bool test_list_tocvector();
	bool test_empiricalCdf();
	bool test_computeCost();
	bool test_informationCriterion();
	bool test_gradientDescendent();
	bool test_pdf_weibull();
	bool test_cdf_weibull();
	bool test_pdf_exponential();
	bool test_cdf_exponential();
	bool test_pdf_pareto();
	bool test_cdf_pareto();
	bool test_pdf_cauchy();
	bool test_cdf_cauchy();
	bool test_fitModels();
	bool test_modelSelection();
#endif

private:
	double min_time = 5e-7;
	double diferential = 4e-14;

#ifdef TEST_FUNCTIONS
	//interarrival sample data
	vec interArrivalSample =
	{ 0, 0, 0.000203, 1e-05, 0.318392, 1.00003, 0.517214, 3.2e-05, 7.6e-05,
			0.159572, 0.028456, 0.000267, 1.6e-05, 0.000362, 0.001436, 0.012243,
			0.052051, 0.010572, 0.300375, 1.6e-05, 0.00012, 0.171993, 0.000161,
			0.180517, 0.336145, 9e-06, 0.007852, 0.044852, 0.020313, 0.030458,
			0.218979, 0.250961, 0.158453, 0.042762, 0.129894, 0.01578, 0.016078,
			3.1e-05, 2.2e-05, 0.000309, 1.8e-05, 7.1e-05, 3e-06, 0.00018, 6e-06,
			0.041497, 0.094336, 0.030568, 0.69278, 0.083745,
			0.09501999999999999, 0.246721, 0.000322, 1.5e-05, 6.4e-05, 0.00031,
			6.600000000000001e-05, 0.000203, 5.4e-05, 4e-06, 4.6e-05,
			0.0005509999999999999, 0.046676, 0.000133, 0.00025, 0.000106, 4e-06,
			6e-05, 0.000579, 0.020548, 0.020144, 0.00017, 4.3e-05, 3.8e-05,
			0.000403, 8.8e-05, 9.2e-05, 2.1e-05, 8.899999999999999e-05, 7.9e-05,
			0.037013, 0.003625, 0.000174, 0.001402, 0.01547, 0.000137, 0.000257,
			0.000181, 0.000249, 0.000175, 0.000262, 0.000188, 0.000245,
			0.000198, 0.000194, 0.039691, 0.377134, 0.025489,
			0.08887399999999999, 0.071201, 0.118396, 0.011301, 0.18306,
			0.471818, 1.2e-05, 0.057105, 0.158384, 0.033412, 0.279247, 0.615341,
			0.07193099999999999, 0.041286, 6e-06, 6.4e-05, 0.000615, 0.000159,
			1.5e-05, 0.179127, 0.003532, 0.087938, 0.128529, 0.003537, 0.555139,
			0.312737, 0.614656, 0.20354, 0.18178, 0.728741, 0.227757, 0.657457,
			1.122729, 0.000405, 1.1e-05, 0.876119, 1.999258, 0.936457, 0.000285,
			1.2e-05, 4.5e-05, 5.3e-05, 0.000321, 0.000137, 0.00026, 0.00031,
			0.000224, 2.5e-05, 0.000171, 3.7e-05, 0.000211, 9.3e-05, 0.000175,
			2.8e-05, 0.000447, 0.000124, 3e-06, 0.000121, 0.000124, 1.3e-05,
			0.000111, 0.000129, 3e-06, 3.6e-05, 3e-06, 1.8e-05, 3.9e-05,
			8.6e-05, 0.00021, 0.000239, 0.000161, 3.2e-05, 0.000224, 0.001489,
			0.00078, 0.000127, 7e-06, 0.000197, 0.000703, 2.5e-05, 0.000145,
			0.00263, 0.001061, 0.001058, 8e-06, 1.5e-05, 4.6e-05, 0.001069,
			0.013118, 6.4e-05, 0.016585, 0.008005, 0.004525, 0.01047, 1.1e-05,
			4e-06, 1e-06, 1e-06, 1e-06, 0.000346, 0.003028, 0.3629, 0.605291,
			0.024532, 0.13292, 6e-06, 2e-06, 2e-06, 2e-06, 2e-06, 3.5e-05,
			0.000585, 0.000348, 0.000227, 0.000171, 7e-06, 0.08057, 0.758265,
			0.999926, 0.026228, 0.430843, 0.460833, 0.076263, 0.682728,
			0.000663, 0.286479, 0.000348, 1.2e-05, 5e-05, 0.000407, 4.8e-05,
			0.000171, 5e-05, 0.000202, 3.5e-05, 0.000215, 2.7e-05, 0.000227,
			3.3e-05, 0.000461, 0.000124, 8e-06, 0.000109, 0.000124, 7e-06,
			0.000115, 0.000123, 1e-06, 0.000122, 0.000125, 3e-06, 0.000122,
			0.000121, 1.8e-05, 0.000105, 3.5e-05, 4e-06, 4.6e-05, 0.000216,
			8.6e-05, 0.000174, 0.000106, 0.00019, 7.8e-05, 0.000171, 7.1e-05,
			0.000256, 0.000219, 0.000174, 8.899999999999999e-05, 0.000168,
			0.000114, 8e-06, 4.5e-05, 0.000549, 0.041588, 9.399999999999999e-05,
			0.000272, 5.8e-05, 4e-06, 4.2e-05, 0.000563, 0.012524, 0.011557,
			0.000133, 0.033325, 8.1e-05, 3.4e-05, 0.000523, 0.041159, 5.2e-05,
			2.3e-05, 5.7e-05, 0.016827, 9.500000000000001e-05, 0.000229,
			7.1e-05, 0.000184, 7.2e-05, 0.000183, 5.3e-05, 0.000166, 3.2e-05,
			0.000225, 0.037247, 1.000281, 1.5e-05, 1.8e-05, 3.9e-05, 7e-06,
			6e-06, 5.9e-05, 8e-06, 4e-06, 4.7e-05, 0.000492, 4.2e-05, 7e-06,
			4e-06, 3e-06, 6e-06, 2.5e-05, 6e-05, 1.8e-05, 6e-06, 1.8e-05, 6e-06,
			5.7e-05, 1.5e-05, 0.000178, 3.4e-05, 6e-06, 4e-06, 4e-06, 7e-06,
			0.000154, 1.6e-05, 4e-06, 3e-06, 4e-06, 5.7e-05, 0.000236, 4.2e-05,
			7e-06, 3e-06, 4e-06, 0.000261, 4.1e-05, 6e-06, 8.3e-05, 4.9e-05,
			8e-06, 5e-06, 4e-06, 4e-06, 5e-06, 1.1e-05, 0.000377, 4.3e-05,
			8e-06, 4e-06, 4e-06, 6e-06, 4e-06, 9e-06, 0.000183, 2.7e-05, 6e-06,
			3e-06, 4e-06, 0.000203, 2.5e-05, 6e-06, 3e-06, 4e-06, 6e-06, 0.0002,
			2.8e-05, 6e-06, 4e-06, 4e-06, 7e-06, 0.000205, 2.7e-05, 6e-06,
			4e-06, 3e-06, 0.000183, 2.9e-05, 6e-06, 4e-06, 3e-06, 7e-06,
			0.000217, 2.6e-05, 6e-06, 4e-06, 3e-06, 6e-06, 0.000209, 3.5e-05,
			6e-06, 3e-06, 3e-06, 0.000247, 4.3e-05, 5e-06, 3e-06, 3e-06, 7e-06,
			8.000000000000001e-05, 1.4e-05, 4e-06, 4e-06, 3e-06, 7.6e-05,
			0.000233, 4.2e-05, 6e-06, 3e-06, 3e-06, 0.000108, 1.4e-05, 5e-06,
			4e-06, 5.5e-05, 0.000259, 3.3e-05, 3.4e-05, 0.000106, 0.000304,
			0.000247, 3.3e-05, 0.000169, 0.000276, 3.3e-05, 3.2e-05, 0.000112,
			0.000275, 0.000284, 0.000244, 6.4e-05, 0.00013, 0.000222, 0.000244,
			0.000377, 0.000136, 0.000324, 6.1e-05, 0.000101, 0.000304, 0.000205,
			0.000298, 0.000202, 0.000265, 4.3e-05, 0.000182, 0.000198, 3.6e-05,
			0.036618, 0.167993, 0.000405, 5.6e-05, 0.132945, 0.5075499999999999,
			1.839101, 0.160079, 0.856059, 0.00019, 0.814619, 0.001748, 0.326707,
			0.016594, 0.104517, 0.000487, 1.2e-05, 3.5e-05, 0.030896, 1.3e-05,
			2.5e-05, 2e-06, 5.6e-05, 2e-06, 0.00016, 8e-06, 1e-06, 4.4e-05,
			6e-06, 0.000232, 2e-06, 9.6e-05, 5e-06, 1e-06, 0.000219, 3e-06,
			1e-06, 0.000281, 4e-06, 1e-06, 0.000247, 2e-06, 1e-06, 0.000182,
			4e-06, 1e-06, 0.000234, 3e-06, 2e-06, 0.000331, 7e-06, 2e-06,
			0.000228, 7e-06, 0.000124, 0.000342, 0.000243, 0.000109, 8e-06,
			0.00015, 6.1e-05, 8e-06, 0.006888, 7.4e-05, 0.033877, 0.03964,
			0.031221, 0.027626, 0.029209, 0.027819, 0.008146, 0.003837,
			0.054232, 0.141787, 0.189477, 0.000731, 0.037917, 0.239763,
			0.016276, 0.101792, 0.121279, 0.569522, 0.162138, 0.167452,
			0.100904, 0.050978, 0.193571, 0.023699, 0.00188, 0.7299330000000001,
			0.14706, 0.069059, 0.000354, 2.1e-05, 0.000376, 0.051512, 4e-05,
			0.00026, 1.9e-05, 5.1e-05, 4e-06, 6e-05, 0.000322, 4.5e-05, 1.3e-05,
			9e-06, 7.8e-05, 3.9e-05, 0.000258, 5.4e-05, 6.9e-05, 8e-06, 6.2e-05,
			0.000148, 4.2e-05, 0.000136, 5.7e-05, 4.7e-05, 0.000226, 0.00012,
			0.000106, 5.4e-05, 0.000169, 9.7e-05, 0.000135, 5.3e-05, 0.000161,
			5.6e-05, 2.9e-05, 1.1e-05, 2.8e-05, 2.9e-05, 0.000329, 4.8e-05,
			2.4e-05, 1e-05, 3.4e-05, 2.8e-05, 0.000345, 0.000111, 0.000216,
			8e-06, 0.010716, 0.000189, 0.002223, 0.000222, 0.000261, 6.1e-05,
			0.000497, 0.000121, 6e-06, 3.1e-05, 7.4e-05, 0.000316, 0.000201,
			0.000241, 0.037019, 0.011593, 0.025272, 1.2e-05, 0.005462, 0.000153,
			0.000308, 1.8e-05, 0.000457, 0.000157, 1.2e-05, 0.000111,
			6.499999999999999e-05, 8.500000000000001e-05, 0.000121, 0.036178,
			0.062789, 0.209764, 2.2e-05, 0.000104, 0.000598, 0.000113, 0.000858,
			0.001237, 1.1e-05, 6.4e-05, 0.022295, 0.002239, 0.02595, 0.048743,
			2.3e-05, 0.048933, 0.000101, 3.8e-05, 0.134329, 0.013961, 1e-05,
			0.003331, 7e-06, 0.006234, 0.000175, 8e-06, 0.0002, 0.000699,
			2.4e-05, 0.001724, 6e-06, 0.000252, 1.3e-05, 0.00269, 0.000125,
			7e-06, 0.000158, 0.007282, 0.015473, 0.064868, 6e-06, 0.216,
			2.2e-05, 2.9e-05, 6.9e-05, 0.000317, 1.4e-05, 0.002346, 0.00051,
			0.005226, 2e-05, 0.019417, 0.001007, 0.000196, 0.005594, 0.005761,
			0.008026, 0.09038, 0.000229, 0.000128, 0.209391, 0.231852, 0.032119,
			5e-06, 5e-06, 2e-06, 2e-06, 1e-06, 2e-06, 3.3e-05, 0.000664, 1e-05,
			2e-06, 0, 1e-06, 1e-06, 8.899999999999999e-05, 1.8e-05, 2.7e-05,
			5e-06, 1e-06, 1e-06, 0.000308, 4e-06, 2e-06, 2e-06, 0.000159, 5e-06,
			1e-06, 1e-06, 0.000537, 8e-06, 2e-06, 2e-06, 1e-06, 1e-06, 1e-06,
			0.00044, 1e-05, 2e-06, 3e-06, 3e-06, 3e-06, 2e-06, 0.0001, 0.000368,
			1e-05, 1e-06, 3e-06, 3e-06, 4e-06, 2e-06, 0.000452, 5e-06, 3e-06,
			2e-06, 2e-06, 2e-06, 0.000215, 6e-06, 2e-06, 2e-06, 0.000259, 7e-06,
			3e-06, 1e-06, 0.000209, 5e-06, 2e-06, 1e-06, 0.000236, 4e-06, 1e-06,
			2e-06, 0.000238, 3e-06, 1e-06, 2e-06, 0.000252, 1.6e-05, 1e-06,
			1e-06, 0.000255, 1.2e-05, 1e-06, 2e-06, 0.000202, 1.5e-05, 1e-06,
			1e-06, 0.000251, 1.2e-05, 1e-06, 1e-06, 0.000198, 4e-06, 1e-06,
			2e-06, 0.000254, 4e-06, 1e-06, 1e-06, 0.00013, 0.000103, 2e-06,
			1e-06, 1e-06, 0.00023, 5e-06, 2e-06, 1e-06, 0.000244, 4e-06, 1e-06,
			0, 0.000233, 6e-06, 1e-06, 1e-06, 0.000241, 4e-06, 1e-06, 1e-06,
			0.000238, 6e-06, 3e-06, 1e-06, 0.000234, 6e-06, 2e-06, 1e-06,
			0.000234, 8e-06, 1e-06, 1e-06, 0.000236, 7e-06, 1e-06, 1e-06,
			0.000236, 4e-06, 1e-06, 1e-06, 0.000237, 5e-06, 1e-06, 2e-06,
			0.000236, 2e-06, 3e-06, 4e-06, 0.000237, 6e-06, 3e-06, 3e-06,
			0.000232, 4e-06, 2e-06, 1e-06, 0.000238, 4e-06, 1e-06, 2e-06,
			0.000239, 6e-06, 1e-06, 1e-06, 0.00024, 2e-06, 1e-06, 1e-06,
			0.000241, 3e-06, 2e-06, 1e-06, 0.000238, 4e-06, 2e-06, 1e-06,
			0.000237, 3e-06, 1e-06, 2e-06, 0.00024, 5e-06, 1e-06, 1e-06,
			0.000237, 4e-06, 1e-06, 1e-06, 0.00024, 3e-06, 2e-06, 1e-06,
			0.000305, 4e-06, 2e-06, 0.000194, 1.2e-05, 1e-06, 0.000223, 1.2e-05,
			1e-06, 0.000235, 1.5e-05, 1e-06, 0.000224, 1.2e-05, 2e-06, 0.000231,
			1.1e-05, 2e-06, 0.000225, 6e-06, 2e-06, 1e-06, 0.000245, 1.4e-05,
			1e-06, 0.000218, 2e-06, 1e-06, 0.000243, 3e-06, 2e-06, 0.00024,
			4e-06, 1e-06, 0.00024, 4e-06, 2e-06, 0.000238, 2e-06, 1e-06, 1e-06,
			0.000241, 2e-06, 1e-06, 0.000242, 4e-06, 1e-06, 0.000241, 2e-06,
			1e-06, 0.000242, 4e-06, 1e-06, 0.00024, 4e-06, 1e-06, 0.000243,
			1.6e-05, 2e-06, 0.000225, 1.2e-05, 2e-06, 0.000231, 1.5e-05,
			0.00023, 1.1e-05, 2e-06, 0.000231, 1.1e-05, 0.000237, 1.4e-05,
			0.000219, 4e-06, 2e-06, 0.000241, 5e-06, 0.000245, 3e-06, 0.000247,
			6e-06, 3e-06, 0.000229, 5e-06, 0.000238, 6e-06, 3e-06, 0.000244,
			1.3e-05, 0.000243, 2e-06, 0.000222, 0.000245, 3e-06, 2e-06,
			0.000239, 3e-06, 3e-06, 0.000238, 1e-06, 1e-06, 0.000244, 3e-06,
			2e-06, 0.000239, 2e-06, 0.000244, 1e-06, 0.000244, 0.000245,
			0.000255, 0.000243, 0.000251, 1.6e-05, 0.000231, 1.3e-05, 0.000228,
			1.2e-05, 0.000238, 5e-06, 0.000235, 1.3e-05, 0.000192, 7.8e-05,
			1.4e-05, 9.8e-05, 9.7e-05, 0.000236, 0.000256, 0.000247, 2e-05,
			8.4e-05, 1.2e-05, 0.000104, 6.9e-05, 1.3e-05, 0.000179, 0.000245,
			0.000256, 1e-06, 9.2e-05, 0.000124, 1.7e-05, 0.000242, 0.000143,
			1.7e-05, 0.064764, 0.268533, 0.000247, 0.000117, 4.6e-05, 0.0001,
			0.00026, 1.3e-05, 7.8e-05, 0.000413, 7.499999999999999e-05,
			0.000187, 7.6e-05, 0.00019, 0.015809, 0.000237, 4e-05, 2.3e-05,
			0.000115, 0.000268, 9e-06, 3.1e-05, 0.000317, 0.000248, 0.00025,
			0.000111, 0.00018, 3.2e-05, 0.000196, 5.3e-05, 0.000188, 4.7e-05,
			0.000197, 6.1e-05, 0.000184, 6.8e-05, 0.000187, 0.03969, 0.401298,
			0.191103, 0.038273, 0.155567, 2.8e-05, 0.000256,
			6.600000000000001e-05, 1.2e-05, 0.000205, 1.5e-05, 0.413707,
			0.02753, 0.142191, 0.120225, 0.761589, 0.148513,
			0.07999199999999999, 0.443199, 1.476141, 0.380089, 0.005695,
			0.000311, 1.2e-05, 0.008484999999999999, 0.849543, 0.00359,
			0.751508, 1.292865, 0.179771, 0.5266, 0.366045, 2e-05, 1.03151,
			1.2e-05, 0.6017670000000001, 0.705453, 1.29376, 0.388697, 0.000119,
			0.000292, 1.4e-05, 7e-06, 0.000235, 1e-05, 0.171773,
			0.07598100000000001, 0.310302, 0.0004, 1.7e-05, 8.1e-05, 0.000385,
			0.000118, 0.000302, 0.000114, 0.000281, 2.9e-05, 0.000209, 2.7e-05,
			0.000239, 2.5e-05, 0.000212, 6e-05, 0.000224, 9.1e-05, 0.000233,
			1.9e-05, 0.000198, 1.9e-05, 0.000289, 0.000267, 0.000198, 6e-05,
			0.000195, 7.6e-05, 3e-06, 2e-06, 1e-06, 2e-06, 2e-06, 6.9e-05,
			0.000555, 0.000244, 0.000241, 0.039375, 0.000124, 0.000288,
			9.899999999999999e-05, 3e-06, 6.2e-05, 0.000571, 0.048884, 0.000149,
			1.4e-05, 6e-06, 6e-06, 3.3e-05, 1.6e-05, 7e-06, 2.5e-05, 0.000324,
			0.000219, 9e-06, 0.000153, 0.000128, 0.00011, 0.000155, 9e-06,
			0.00017, 7.3e-05, 0.037596, 0.027548, 0.000148, 0.00018, 0.016617,
			7.499999999999999e-05, 0.000234, 3.3e-05, 0.000171, 2.8e-05,
			0.000231, 3.5e-05, 0.00019, 2.8e-05, 0.000209, 0.039791, 0.832296,
			1.999254, 0.048807, 0.016688, 0.103137, 0.855847, 0.655344,
			0.000661, 0.013452, 0.168439, 0.136821, 0.025272, 9e-06,
			0.007948999999999999, 0.037041, 0.047981, 0.008460000000000001,
			0.04047, 0.009528, 0.020497, 0.072979, 0.009079, 0.11388, 0.023627,
			0.016839, 0.235363, 0.200238, 0.259802, 0.133895, 0.19443, 0.003572,
			0.124423, 0.003563, 0.37003, 0.040397, 0.3915, 0.000331, 1.7e-05,
			0.186427, 0.003647, 0.000144, 0.004702, 0.542918, 0.227775,
			0.641863, 0.034463, 0.016594, 0.419727, 0.000376, 3.6e-05, 0.000115,
			0.000327, 0.0005330000000000001, 0.00028, 0.000207, 0.000324,
			0.000296, 1e-05, 7e-06, 4.2e-05, 0.000562, 0.010122, 3.8e-05,
			0.018846, 0.000187, 0.000264, 0.000181, 0.000292, 0.000118,
			0.000469, 0.01084, 0.000237, 3.8e-05, 0.019239, 2.8e-05, 0.000227,
			0.0001, 0.000213, 0.000112, 0.000241, 0.000121, 0.000312, 0.000149,
			0.000209, 0.000133, 0.000296, 0.000132, 0.000214, 0.00166, 0.02373,
			0.001922, 4e-06, 4.3e-05, 3e-06, 5.1e-05, 3e-06, 5.6e-05, 2e-06,
			5.4e-05, 1e-06, 0.000392, 8e-06, 1e-06, 4.8e-05, 3e-06, 3.7e-05,
			2e-06, 0.000158, 1e-05, 2e-06, 1e-06, 4.7e-05, 0.000205, 3e-06,
			1e-06, 1e-06, 0.000516, 0.000201, 0.000213, 0.000308, 0.000241,
			0.00026, 0.000234, 0.169798, 8.500000000000001e-05, 0.0003,
			0.000144, 3e-06, 2e-06, 6.8e-05, 0.000589, 0.03856, 0.035385,
			0.000115, 0.000179, 5.8e-05, 0.000143, 0.000343, 0.000129, 0.000214,
			7.2e-05, 0.000288, 8.1e-05, 0.000203, 5.1e-05, 0.000369,
			9.500000000000001e-05, 0.000176, 0.001365, 0.013006, 0.001368,
			3e-06, 1e-06, 1e-06, 1e-06, 2e-06, 5.2e-05, 2e-06, 6.3e-05, 1.3e-05,
			1e-06, 1e-06, 6.7e-05, 2e-06, 1e-06, 4.3e-05, 4e-06, 6.1e-05, 1e-06,
			1e-06, 7.1e-05, 2e-06, 7e-06, 0.0002, 0.000255, 0.000229, 0.000516,
			0.000282, 0.000244, 0.000256, 0.000238, 0.000242, 0.000245,
			0.029151, 8.3e-05, 0.000294, 0.000143, 5e-06, 2e-06, 6.4e-05,
			0.000604, 0.012818, 0.00022, 0.019331, 0.000117, 0.000224, 0.000222,
			0.000234, 7.6e-05, 0.000216, 6.7e-05, 0.00028, 7.1e-05, 0.000219,
			4.5e-05, 0.000249, 6.7e-05, 0.000193, 0.001358, 0.013339, 0.001356,
			4e-06, 1e-06, 1e-06, 1e-06, 2e-06, 1e-06, 3.5e-05, 1e-06, 3.2e-05,
			6e-06, 6.8e-05, 1e-06, 1e-06, 4e-05, 7e-06, 1.5e-05, 2.7e-05, 5e-06,
			3.2e-05, 6e-06, 2.5e-05, 3e-05, 0.000278, 0.000251, 0.000215,
			0.000489, 0.000514, 0.000257, 0.000257, 0.000243, 0.000218,
			0.010181, 0.001256, 3e-05, 0.017594, 6.499999999999999e-05,
			0.000246, 0.000173, 4e-06, 1e-06, 6.499999999999999e-05, 0.000554,
			0.032475, 0.000117, 0.000258, 0.00054, 0.000265, 0.000146, 0.000199,
			6.9e-05, 0.000292, 0.000126, 0.000205, 0.000109, 0.000287, 0.000119,
			0.00021, 0.001505, 0.01251, 0.001603, 3e-06, 2e-06, 1e-06, 1e-06,
			2e-06, 1e-06, 4.5e-05, 3e-06, 6.7e-05, 1.2e-05, 1e-06, 1e-06,
			6.7e-05, 1.9e-05, 1e-06, 1e-06, 1e-05, 1.9e-05, 5.9e-05, 2e-06,
			9.3e-05, 2e-06, 0.000219, 0.000245, 0.000208, 0.000262, 0.000498,
			0.000251, 0.000244, 0.000246, 0.000244, 0.000247, 0.02889, 8.1e-05,
			0.000216, 0.000107, 3e-06, 2e-06, 6.9e-05, 0.0005820000000000001,
			0.01628, 0.00018, 5.4e-05, 0.000269, 0.015699, 2.2e-05, 0.000102,
			0.000347, 0.00041, 0.000227, 7.7e-05, 0.0002, 6.2e-05, 0.000236,
			4.8e-05, 0.00018, 7.499999999999999e-05, 0.000302, 9.2e-05,
			0.000167, 0.001491, 0.012937, 0.001596, 3e-06, 2e-06, 2e-06, 1e-06,
			2e-06, 2e-06, 2e-06, 1e-06, 3.4e-05, 3e-06, 7.499999999999999e-05,
			1.8e-05, 1e-06, 1e-06, 1e-06, 6e-05, 2e-06, 9e-06, 5.3e-05, 5e-06,
			2e-05, 1e-05, 0.000295, 0.000242, 0.000245, 0.000522, 0.000488,
			0.000247, 0.000243, 0.000246, 0.000246, 0.028895, 8.2e-05, 0.000303,
			0.000144, 5e-06, 2e-06, 6.8e-05, 0.000595, 0.009254, 0.027474,
			0.012253, 0.000107, 0.000184, 5.8e-05, 0.000119, 0.000216, 4e-05,
			0.000201, 2.2e-05, 0.000263, 4.1e-05, 0.000208,
			6.999999999999999e-05, 0.000266, 3.7e-05, 0.000167, 0.001494,
			0.013389, 0.001558, 4e-06, 2e-06, 1e-06, 2e-06, 1e-06, 1e-06,
			3.8e-05, 3e-06, 6.1e-05, 1e-06, 1e-06, 4.2e-05, 4e-06, 5.6e-05,
			4e-06, 1e-06, 3e-05, 8e-06, 5.4e-05, 1e-06, 1e-06, 2.6e-05,
			0.000256, 0.000241, 0.000244, 0.000553, 0.000246, 0.00022, 0.00024,
			0.000244, 0.000247, 0.000213, 0.028935, 8.899999999999999e-05,
			0.000214, 8.2e-05, 4e-06, 2e-06, 5.9e-05, 0.000562, 0.032548,
			0.000119, 0.000226, 0.0003, 0.000213, 4e-05, 0.000167, 3.3e-05,
			0.000333, 0.000132, 0.000208, 9.1e-05, 0.000296, 0.000118, 0.000181,
			0.00152, 0.013032, 0.001587, 4e-06, 2e-06, 1e-06, 2e-06, 2e-06,
			2e-06, 3.8e-05, 2e-06, 8.1e-05, 1e-06, 0, 1.6e-05, 1e-06, 5e-05,
			4e-06, 5.6e-05, 1e-06, 1e-06, 3.1e-05, 9e-06, 2.8e-05, 9e-06,
			0.00026, 0.000242, 0.000249, 0.000486, 0.000245, 0.000245, 0.000348,
			0.000419, 0.000279, 0.028802, 8.3e-05, 0.000286, 0.000115, 4e-06,
			2e-06, 5.9e-05, 0.000559, 0.000241, 0.002101, 3.5e-05, 0.030066,
			0.000106, 0.000205, 0.001807, 0.000205, 7.2e-05, 0.000191, 3e-05,
			0.000302, 0.000137, 0.000201, 8.899999999999999e-05, 0.000306,
			0.000116, 0.000325, 0.001743, 0.011198, 0.001645, 3e-06, 2e-06,
			2e-06, 1e-06, 2e-06, 1e-06, 1e-05, 3.5e-05, 2e-06, 6.7e-05, 2e-06,
			0, 4.5e-05, 8e-06, 2.2e-05, 3.3e-05, 8e-06, 7.1e-05, 1.4e-05, 1e-06,
			1e-06, 2.7e-05, 0.000255, 0.00024, 0.000243, 0.0005240000000000001,
			0.000242, 0.000246, 0.000282, 0.000452, 0.000251, 0.045419,
			8.500000000000001e-05, 0.000254, 6.1e-05, 2e-06, 3e-06, 6.9e-05,
			0.000559, 0.032483, 7.2e-05, 0.000221, 0.000143, 0.000211, 7.4e-05,
			0.0002, 4.9e-05, 0.000301, 0.000129, 0.000212, 9.2e-05, 0.000262,
			6e-05, 0.000198, 0.001624, 0.013169, 0.001673, 3e-06, 1e-06, 1e-06,
			2e-06, 2e-06, 1e-06, 4e-05, 3e-06, 6.4e-05, 1e-06, 1e-06, 4.8e-05,
			6e-06, 1.2e-05, 6.3e-05, 2e-06, 5.2e-05, 2e-06, 1e-06, 0, 3.5e-05,
			1e-06, 0.000295, 0.000239, 0.000243, 0.000492, 0.000243, 0.000278,
			0.00031, 0.000424, 0.000219, 0.028386, 0.000228, 5.4e-05, 0.000213,
			3.5e-05, 0.000238, 0.007959000000000001, 2.2e-05, 7.1e-05, 0.000481,
			0.000111, 4e-06, 1e-06, 6.8e-05, 0.000623, 0.038715, 0.010124,
			0.000116, 0.000228, 6.2e-05, 0.002913, 0.000265, 2.1e-05, 0.000179,
			1.5e-05, 1.3e-05, 0.000229, 3.9e-05, 0.000196, 3.5e-05, 0.000269,
			7.3e-05, 0.000191, 5.3e-05, 0.000247, 2.2e-05, 0.000177, 0.001569,
			0.010022, 0.001705, 3e-06, 2e-06, 1e-06, 1e-06, 3e-06, 1e-06, 1e-06,
			1e-06, 3.6e-05, 2e-06, 6.499999999999999e-05, 1e-06, 1e-06, 6e-05,
			1e-06, 1e-06, 5.6e-05, 1e-06, 4.2e-05, 4e-06, 1.7e-05, 2e-06,
			0.000327, 0.000232, 0.00025, 0.000239, 0.000501, 0.000215, 0.000245,
			0.000244, 0.000246, 0.000325, 0.02873, 8.7e-05, 0.000337, 0.000112,
			3e-06, 2e-06, 5.5e-05, 0.00056, 0.032383, 0.000119, 0.000246,
			0.001701, 0.000251, 0.004701, 0.000195, 0.006657, 0.027981, 7.3e-05,
			0.00024, 0.019186, 0.000333, 7.7e-05, 0.000241, 0.037189, 0.272651,
			0.227517, 6e-06, 2e-06, 2e-06, 1e-06, 2e-06, 3.7e-05, 2e-06,
			3.4e-05, 2e-06, 0.0006089999999999999, 1.8e-05, 3e-06,
			6.499999999999999e-05, 3e-06, 5.6e-05, 9e-06, 3e-06, 1e-06,
			6.499999999999999e-05, 5e-06, 1e-06, 0.000207, 9e-06, 2e-06, 1e-06,
			1e-06, 0.000442, 9e-06, 1e-06, 2e-06, 1e-06, 1e-06, 3.5e-05,
			0.000105, 1.2e-05, 7.499999999999999e-05, 3e-06, 1e-06, 1e-06,
			1e-06, 1e-06, 1e-06, 0.00047, 3e-06, 2e-06, 1e-06, 1e-06, 1e-06,
			1e-06, 0.000255, 4e-06, 1e-06, 2e-06, 1e-06, 1e-06, 2e-06, 0.000462,
			4e-06, 2e-06, 1e-06, 1e-06, 1e-06, 1e-06, 0.000234, 4e-06, 1e-06,
			1e-06, 1e-06, 0.00024, 6e-06, 1e-06, 2e-06, 1e-06, 0.000236, 5e-06,
			1e-06, 1e-06, 1e-06, 0.000256, 4e-06, 1e-06, 1e-06, 1e-06, 0.00022,
			2e-06, 1e-06, 0.000242, 4e-06, 2e-06, 0.000241, 5e-06, 2e-06,
			0.000233, 3e-06, 2e-06, 0.000239, 5e-06, 2e-06, 0.000238, 4e-06,
			1e-06, 0.00024, 5e-06, 1e-06, 0.000239, 3e-06, 1e-06, 0.000245,
			0.000279, 0.000142, 0.000114, 8.7e-05, 5e-06, 0.000157, 0.000225,
			0.000241, 6.2e-05, 0.000185, 6.2e-05, 4e-06, 0.000144, 0.000247,
			0.000255, 6.600000000000001e-05, 0.00017, 7.499999999999999e-05,
			1.5e-05, 0.000168, 0.000258, 6.8e-05, 0.000165, 7.7e-05, 5e-06,
			0.000145, 0.000242, 6.8e-05, 0.00018, 9.8e-05, 5e-06, 0.000169,
			0.000265, 0.000232, 5.3e-05, 2.5e-05, 5e-06, 0.009259, 0.039637,
			0.00865, 2.8e-05, 5e-06, 5e-06, 0.009263, 2.7e-05, 0.087632, 6e-05,
			0.000318, 1.4e-05, 1.6e-05, 2e-06, 5e-06, 4.2e-05, 0.00032,
			0.037569, 0.703033, 0.214916, 0.64994, 0.04462, 0.302059, 0.999951,
			0.137821, 0.039703, 0.027832, 0.000366, 1.7e-05, 0.000111, 0.000346,
			0.000131, 0.00019, 4.5e-05, 4e-06, 2e-06, 3.8e-05, 0.000618,
			0.036949, 0.0052, 0.000138, 0.0002, 0.000107, 9.399999999999999e-05,
			4e-06, 5.9e-05, 0.000579, 0.032121, 0.000118, 1.5e-05, 3.3e-05,
			3e-06, 0.000374, 0.000259, 6e-06, 0.00016, 5.3e-05,
			6.600000000000001e-05, 0.036463, 0.00435, 0.000131, 0.0002,
			0.016695, 0.000124, 0.000254, 7.7e-05, 0.000202, 6.9e-05, 0.000174,
			4.5e-05, 0.000229, 6.7e-05, 0.000176, 0.038503, 0.271061, 0.036943,
			0.003036, 0.307457, 0.23321, 0.029976, 0.000198, 0.62617, 3e-05,
			0.000353, 1.9e-05, 1e-05, 2e-06, 0.109966, 0.651886, 0.213682,
			4.9e-05, 2.7e-05, 0.000236, 1e-05, 4.6e-05, 2e-06, 0.00011, 2e-06,
			0.000152, 1e-06, 0.133729, 0.609789, 0.031985, 2.2e-05, 9.6e-05,
			0.016657, 9.1e-05, 0.000312, 0.000155, 0.000175, 3.6e-05, 0.000326,
			0.000222, 5e-06, 4.3e-05, 4e-06, 3.9e-05, 1.9e-05, 2.4e-05, 2.7e-05,
			2e-06, 5.9e-05, 0.000446, 2e-05, 3e-06, 5.8e-05, 2e-06, 6e-05,
			3e-06, 5.8e-05, 1e-05, 3e-06, 2e-06, 6.2e-05, 0.000145, 4e-06,
			1e-06, 1e-06, 3.7e-05, 0.000475, 4e-06, 2e-06, 1e-06, 2e-06,
			3.5e-05, 9e-06, 3.5e-05, 1e-06, 0.00041, 8e-06, 2e-06, 1e-06, 1e-06,
			2e-06, 1e-06, 3.9e-05, 2e-06, 0.000412, 4e-06, 2e-06, 1e-06, 1e-06,
			2e-06, 1e-06, 1e-06, 3.9e-05, 0.000437, 4e-06, 2e-06, 1e-06, 2e-06,
			1e-06, 1e-06, 2e-06, 1e-06, 0.000476, 4e-06, 2e-06, 1e-06, 2e-06,
			1e-06, 1e-06, 1e-06, 1e-06, 0.000455, 5e-06, 1e-06, 1e-06, 1e-06,
			1e-06, 1e-06, 0.000236, 1e-06, 2e-06, 1e-06, 1e-06, 0.000242, 5e-06,
			2e-06, 1e-06, 0.000302, 7e-06, 1e-06, 2e-06, 0.000174, 4e-06, 2e-06,
			1e-06, 0.000263, 3e-06, 2e-06, 1e-06, 0.000206, 5e-06, 1e-06, 2e-06,
			0.000263, 9e-06, 2e-06, 1e-06, 0.000241, 1.1e-05, 2e-06, 1e-06,
			0.000262, 1.9e-05, 3e-06, 1e-06, 0.000223, 1.9e-05, 0.000224,
			0.000248, 0.000244, 0.000217, 0.00024, 0.000244, 0.000246, 0.000221,
			0.000274, 0.000241, 0.000212, 0.000277, 0.000213, 0.000281,
			0.000242, 0.000245, 0.000244, 0.000246, 0.000244, 0.00025, 0.00024,
			0.000247, 0.000244, 0.000242, 0.001342, 0.000379, 0.034867,
			0.000105, 0.000224, 9.3e-05, 0.000182, 2.7e-05, 0.00026, 3.4e-05,
			0.000379, 0.039497, 0.5442, 0.357434, 1.099135, 0.041655, 0.85846,
			0.09723900000000001, 0.311746, 0.29389, 0.394283, 0.308007,
			0.445888, 0.148214, 0.097913, 0.307962, 0.692039, 0.307906,
			0.593507, 0.056654, 0.041922, 0.033815, 0.274117,
			0.8501919999999999, 4.8e-05, 4e-05, 5.2e-05, 0.000176, 3.5e-05,
			7.8e-05, 4.1e-05, 3.9e-05, 1.4e-05, 3.8e-05, 2e-05, 1e-05, 5e-06,
			3e-05, 1.3e-05, 0.14915, 0.026055, 0.003791, 0.562937, 1.999277,
			1.065992, 0.000143, 0.523873, 0.311931, 0.028066, 0.069262,
			0.239786, 1.759471, 0.246945, 0.756221, 0.697726, 0.000743,
			0.297642, 0.003465, 0.913362, 1.3e-05, 0.0866, 0.081234, 0.025897,
			0.050497, 0.259128, 0.014569, 0.564476, 0.004333, 0.312601,
			0.003488, 0.683764, 0.995133, 0.004903, 0.270456,
			0.6224730000000001, 0.000158, 0.161523, 0.000449, 2e-05, 0.000129,
			0.000428, 0.000134, 0.000275, 0.000131, 0.000373, 8.8e-05, 0.000344,
			7.6e-05, 0.000289, 9.899999999999999e-05, 0.000266, 0.00011,
			0.000218, 0.000129, 0.000203, 3.2e-05, 0.000265,
			8.899999999999999e-05, 0.000232, 7.9e-05, 0.000238, 0.000368,
			0.000233, 0.000121, 0.000228, 0.000164, 5e-06, 3e-06, 2e-06, 1e-06,
			8.8e-05, 0.000565, 0.000216, 0.034407, 0.000127, 0.000216, 5.8e-05,
			3e-06, 5.7e-05, 0.000566, 0.032524, 0.000164, 1.9e-05, 8e-06,
			2.2e-05, 1.4e-05, 1.4e-05, 4.4e-05, 0.000401, 5.6e-05, 9.6e-05,
			7e-06, 0.000133, 8e-06, 0.000248, 1.3e-05, 1e-06, 0.000302, 7.6e-05,
			0.036992, 0.028077, 0.000239, 0.000188, 0.016701, 0.000103,
			0.000332, 0.000134, 0.000307, 0.00013, 0.000323, 0.000139, 0.000212,
			0.000116, 0.000175, 0.037659, 0.132015, 0.312029, 0.297586,
			1.336091, 0.663151, 1.295715, 0.703577, 0.296261, 0.101385, 0.22987,
			0.000233, 0.434696, 0.000419, 3.6e-05, 0.000159, 0.000332, 0.000899,
			0.000119, 0.00019, 0.0352, 0.144224, 0.05227, 0.149337, 0.185305,
			0.000393, 0.184107, 5.8e-05, 0.000343, 0.183449, 0.000685, 0.000284,
			1.5e-05, 0.000102, 0.000448, 0.00037, 1.8e-05, 0.000112, 0.000401,
			0.00012, 0.00029, 0.000119, 0.000243, 6.4e-05, 0.000177, 5.4e-05,
			0.000188, 5.4e-05, 0.000279, 9.2e-05, 0.000504, 0.000166, 1.5e-05,
			5.4e-05, 0.000175, 1.5e-05, 7.7e-05, 0.000155, 1.5e-05, 0.000101,
			0.000401, 0.000268, 0.000207, 0.0001, 0.000145,
			0.0009700000000000001, 7.2e-05, 0.036994, 0.14733, 0.010867,
			0.037774, 0.056528, 0.10916, 0.000384, 1e-05, 2.9e-05, 0.000265,
			0.037653, 0.175573, 1.9e-05, 0.676889, 0.702174, 0.297846, 1.701447,
			0.066121, 5e-06, 5e-06, 2e-06, 2e-06, 3e-06, 1e-06, 2e-06, 3e-06,
			2e-06, 0.000725, 1.9e-05, 2e-06, 1e-06, 1e-06, 1e-06, 1e-06,
			6.999999999999999e-05, 2e-05, 2e-06, 3e-06, 1e-06, 1e-06, 1e-06,
			0.000218, 7e-06, 1e-06, 1e-06, 1e-06, 7.2e-05, 0.000155, 6e-06,
			1e-06, 1e-06, 1e-06, 0.000256, 3e-06, 2e-06, 1e-06, 0.000559,
			1.6e-05, 0.000227, 0.000299, 2.6e-05, 0.00012, 0.000264, 1.5e-05,
			0.000238, 1.5e-05, 1.4e-05, 0.000224, 0.000234, 1.5e-05, 0.000223,
			0.006457, 2.2e-05, 0.016599, 0.07625899999999999, 0.238719,
			0.000408, 0.037377, 0.08400299999999999, 0.003761, 0.124235,
			0.003565, 1.337726, 1.999245, 1.075482, 2.9e-05, 0.000378, 2e-05,
			9e-06, 2e-06, 1.8e-05, 6e-06, 0.923339, 1.999252, 1.10893, 0.003052,
			0.887231, 1.999357, 1.735131, 0.000561, 0.263547, 1.066412,
			0.000189, 6.7e-05, 0.932557, 0.069161, 0.000414, 1.7e-05, 0.000328,
			0.076215, 0.148162, 1.6e-05, 0.000117, 0.119362, 0.000372, 1.7e-05,
			0.7902090000000001, 0.794934, 0.918648, 0.0002, 1.080404, 0.413585,
			3.6e-05, 0.000285, 5.5e-05, 4e-06, 2.5e-05, 5e-06, 0.091326,
			0.003882, 0.572217, 7e-06, 4e-06, 2e-06, 2e-06, 8e-06, 4.3e-05,
			2e-06, 2e-06, 3.3e-05, 0.000581, 2.4e-05, 3e-06, 2e-06, 2e-06,
			2e-06, 2e-06, 0.000154, 6e-06, 2e-06, 2e-06, 2e-06, 0.000242,
			1.4e-05, 3e-06, 6.600000000000001e-05, 9e-06, 0.00015, 6.8e-05,
			2e-05, 0.000135, 6.7e-05, 1.3e-05, 0.000447, 6.3e-05, 6e-06,
			0.000211, 5e-06, 0.000197, 0.000266, 1.8e-05, 1.8e-05, 0.000206,
			5.2e-05, 1e-05, 0.070046, 1.1e-05, 0.844631, 1.999285, 0.37526,
			0.269128, 0.000144, 0.428814, 0.000377, 2e-05, 0.000132, 0.000335,
			0.000146, 0.00032, 0.00014, 0.000315, 9.000000000000001e-05,
			0.000326, 7.8e-05, 0.000348, 0.000138, 0.000309, 0.000191, 0.000269,
			9.2e-05, 0.000228, 8.1e-05, 0.000333, 0.000478, 0.000208, 0.000122,
			0.000251, 0.000158, 6e-06, 3e-06, 2e-06, 2e-06, 3.7e-05, 0.000592,
			0.000243, 0.026904, 0.000156, 0.000307, 0.000234, 1.1e-05, 5.5e-05,
			0.000566, 0.032068, 0.000263, 5.9e-05, 4.3e-05, 4.5e-05, 4.2e-05,
			4.2e-05, 4e-05, 0.000202, 0.000138, 3.5e-05, 2.2e-05, 3e-05,
			0.000149, 0.000149, 3e-05, 5.4e-05, 0.000164, 2.8e-05, 0.000127,
			0.03696, 0.019837, 0.00032, 0.000252, 0.016369, 0.000222, 0.000284,
			0.000233, 0.000254, 0.00022, 0.000266, 0.000239, 0.000251, 0.000192,
			0.000192, 0.037031, 1.7e-05, 0.116766, 0.192277, 0.435377, 0.091586,
			3.1e-05, 0.000373, 1.9e-05, 1e-05, 2e-06, 0.000261, 1.4e-05,
			0.280014, 0.023348, 3.2e-05, 2.9e-05, 0.000314, 2.2e-05, 1.2e-05,
			2e-06, 0.000253, 1.7e-05, 0.975979, 0.62699, 0.37293,
			0.9999749999999999, 0.626341, 0.492913, 0.014529, 0.570348,
			0.076026, 0.235104, 0.003504, 0.606799, 0.126116, 0.99708, 0.306968,
			0.005815, 0.563343, 0.123881, 0.303074, 0.697012, 0.303014,
			0.075444, 0.000103, 0.496756, 0.124502, 0.303213,
			0.06970899999999999, 0.6271060000000001, 0.303057, 0.571645,
			0.428396, 1.129816, 0.441094, 1.216598, 0.184097, 0.598553,
			1.762639, 0.000133, 0.236455, 0.700106, 0.025727, 0.876292,
			0.003488, 0.124494, 0.00356, 0.265654, 0.031362, 0.997795,
			0.9701149999999999, 0.029865, 0.913193, 0.000163,
			0.08662400000000001, 0.969381, 0.030588, 1.000004, 0.968747,
			1.999191, 0.181717, 0.003031, 0.476371, 0.000189, 0.498117,
			0.001757, 0.094473, 0.070796, 0.257284, 0.415613, 0.324319,
			0.103689, 3e-05, 0.000304, 1.9e-05, 1.3e-05, 1e-06,
			0.09349499999999999, 2.2e-05, 0.042766, 0.496198, 0.263337,
			0.138729, 0.028411, 0.088029, 0.031219, 0.388701, 0.324931,
			0.266295, 0.268827, 0.235355, 0.105707, 1.7e-05, 0.123881, 0.381654,
			0.076361, 0.09857399999999999, 0.11765, 0.07213, 0.144328, 0.047985,
			0.061286, 0.019094, 0.083081, 0.000315, 1.3e-05, 0.171753, 0.072075,
			0.152366, 0.031976, 0.46934, 0.197771, 0.036378 };
#endif

};

int main()
{
	DataProcessor dp;
	dp.printTestResult("QuickSort", dp.testQuickSort());
	dp.printTestResult("List Mode", dp.testMode());
	dp.printTestResult("ListToVector", dp.test_list_tocvector());
	dp.printTestResult("Empirical CDF", dp.test_empiricalCdf());
	dp.printTestResult("Cost Function", dp.test_computeCost());

	dp.printTestResult("Weibull PDF", dp.test_pdf_weibull());
	dp.printTestResult("Weibull CDF", dp.test_cdf_weibull());
	dp.printTestResult("Gradient Descendent", dp.test_gradientDescendent());

	dp.printTestResult("Information Criterion", dp.test_informationCriterion());
	dp.printTestResult("Exponential PDF", dp.test_pdf_exponential());
	dp.printTestResult("Exponential CDF", dp.test_cdf_exponential());
	dp.printTestResult("PDF Pareto", dp.test_pdf_pareto());
	dp.printTestResult("Pareto CDF", dp.test_cdf_pareto());
	dp.printTestResult("PDF Cauchy", dp.test_pdf_cauchy());
	dp.printTestResult("CDF Cauchy", dp.test_cdf_cauchy());
	dp.printTestResult("All fitting", dp.test_fitModels());
	dp.printTestResult("Model selection", dp.test_modelSelection());

	return 0;
}

/*******************************************************************************
 * ACTUAL FUNCTIONS
 ******************************************************************************/

StochasticModelFit* DataProcessor::fitModels(list<double>& empiricalData,
		const string& criterion)
{
	//constants
	const int numberOfModels = 8;
	const int m = empiricalData.size(); //empirical data-size
	//vars
	int counter = 0;
	StochasticModelFit* modelVet = NULL;
	vec paramVec = zeros<vec>(2);
	vec infoCriterion = zeros<vec>(2);

	modelVet = new StochasticModelFit[numberOfModels];

	//Inter-arrival vec
	vec interArrival = zeros<vec>(m);
	counter = 0;
	for (list<double>::iterator it = empiricalData.begin();
			it != empiricalData.end(); it++)
	{
		interArrival(counter) = *it + min_time;
		counter++;
	}
	//Empirical CDF of interArrival
	vec* interArrivalCdf = empiricalCdf(empiricalData);

	//Weibull
	weibullFitting(interArrival, *interArrivalCdf, paramVec, infoCriterion);
	modelVet[0].aic = infoCriterion(0);
	modelVet[0].bic = infoCriterion(1);
	modelVet[0].modelName = "weibull";
	modelVet[0].param1 = paramVec(0);
	modelVet[0].param2 = paramVec(1);
	modelVet[0].size = numberOfModels;

	//normal
	normalFitting(interArrival, paramVec, infoCriterion);
	modelVet[1].aic = infoCriterion(0);
	modelVet[1].bic = infoCriterion(1);
	modelVet[1].modelName = "normal";
	modelVet[1].param1 = paramVec(0);
	modelVet[1].param2 = paramVec(1);
	modelVet[1].size = numberOfModels;

	//exponential mean
	exponentialMeFitting(interArrival, paramVec, infoCriterion);
	modelVet[2].aic = infoCriterion(0);
	modelVet[2].bic = infoCriterion(1);
	modelVet[2].modelName = "exponential-mean";
	modelVet[2].param1 = paramVec(0);
	modelVet[2].param2 = paramVec(1);
	modelVet[2].size = numberOfModels;

	//exponential Linear Regression (LR)
	exponentialLrFitting(interArrival, *interArrivalCdf, paramVec,
			infoCriterion);
	modelVet[3].aic = infoCriterion(0);
	modelVet[3].bic = infoCriterion(1);
	modelVet[3].modelName = "exponential-linear-regression";
	modelVet[3].param1 = paramVec(0);
	modelVet[3].param2 = paramVec(1);
	modelVet[3].size = numberOfModels;

	//pareto linear regression
	paretoLrFitting(interArrival, *interArrivalCdf, paramVec, infoCriterion);
	modelVet[4].aic = infoCriterion(0);
	modelVet[4].bic = infoCriterion(1);
	modelVet[4].modelName = "pareto-linear-regression";
	modelVet[4].param1 = paramVec(0);
	modelVet[4].param2 = paramVec(1);
	modelVet[4].size = numberOfModels;

	//pareto maximum likehood
	paretoMlhFitting(interArrival, *interArrivalCdf, paramVec, infoCriterion);
	modelVet[5].aic = infoCriterion(0);
	modelVet[5].bic = infoCriterion(1);
	modelVet[5].modelName = "pareto-maximum-likehood";
	modelVet[5].param1 = paramVec(0);
	modelVet[5].param2 = paramVec(1);
	modelVet[5].size = numberOfModels;

	//Cauchy
	cauchyFitting(interArrival, *interArrivalCdf, paramVec, infoCriterion);
	modelVet[6].aic = infoCriterion(0);
	modelVet[6].bic = infoCriterion(1);
	modelVet[6].modelName = "cauchy";
	modelVet[6].param1 = paramVec(0);
	modelVet[6].param2 = paramVec(1);
	modelVet[6].size = numberOfModels;

	//Constant
	constantFitting(interArrival, paramVec, infoCriterion);
	modelVet[7].aic = infoCriterion(0);
	modelVet[7].bic = infoCriterion(1);
	modelVet[7].modelName = "constant";
	modelVet[7].param1 = paramVec(0);
	modelVet[7].param2 = paramVec(1);
	modelVet[7].size = numberOfModels;

	if (criterion == "bic")
	{
		qsort(modelVet, numberOfModels, sizeof(StochasticModelFit), compareBic);
	}
	else if (criterion == "aic")
	{
		qsort(modelVet, numberOfModels, sizeof(StochasticModelFit), compareAic);
	}
	else
	{
		cout
				<< "Error @ DataProcessor::fitModels -> Invalid criterion argument: "
				<< criterion << endl;
		cout << "AIC set as default" << endl;
		qsort(modelVet, numberOfModels, sizeof(StochasticModelFit), compareAic);

	}

	delete interArrivalCdf;

	return (modelVet);
}

inline void DataProcessor::weibullFitting(const vec& interArrival,
		const vec& interArrivalCdf, vec& paramVec, vec& infoCriterion)
{
	//init
	int iterations = 1500;
	double learning_rate = 0.01;
	vec theta = zeros(2);
	vec J_history = zeros(iterations);

	//linearization
	vec y = log(-log(1.0 + diferential - interArrivalCdf));
	vec x = log(interArrival);
	mat* X = featureMatrix(x);

	//gradient descendent
	gradientDescendent(*X, y, learning_rate, iterations, theta, J_history);

	//parameter evaluation
	double weibull_alpha = theta(1);
	double weibull_betha = exp(-theta(0) / theta(1));
	paramVec(0) = weibull_alpha;
	paramVec(1) = weibull_betha;

	delete X;

	//information criterion
	double aic = informationCriterion(interArrival, "weibull", paramVec, "aic");
	double bic = informationCriterion(interArrival, "weibull", paramVec, "bic");
	infoCriterion(0) = aic;
	infoCriterion(1) = bic;

}

inline void DataProcessor::normalFitting(const vec& interArrival, vec& paramVec,
		vec& infoCriterion)
{
	double sigma = stddev(interArrival);
	double mu = mean(interArrival);
	paramVec(0) = mu;
	paramVec(1) = sigma;

	//information criterion
	double aic = informationCriterion(interArrival, "normal", paramVec, "aic");
	double bic = informationCriterion(interArrival, "normal", paramVec, "bic");
	infoCriterion(0) = aic;
	infoCriterion(1) = bic;
}

inline void DataProcessor::exponentialLrFitting(const vec& interArrival,
		const vec& interArrivalCdf, vec& paramVec, vec& infoCriterion)
{
	//init
	int iterations = 1500;
	double learning_rate = 0.01;
	vec theta = zeros<vec>(2);
	vec J_history = zeros<vec>(iterations);

	//linearization
	vec y = log(1.00 + diferential - interArrivalCdf);
	//vec y = log(1.00 - interArrivalCdf);
	vec x = interArrival;
	mat* X = featureMatrix(x);

	//gradient descendent
	gradientDescendent(*X, y, learning_rate, iterations, theta, J_history);

	//parameter evaluation
	double exp_lambda = -theta(1);
	paramVec(0) = exp_lambda;
	paramVec(1) = 0;

	delete X;

	//information criterion
	double aic = informationCriterion(interArrival, "exponential", paramVec,
			"aic");
	double bic = informationCriterion(interArrival, "exponential", paramVec,
			"bic");
	infoCriterion(0) = aic;
	infoCriterion(1) = bic;
}

inline void DataProcessor::exponentialMeFitting(const vec& interArrival,
		vec& paramVec, vec& infoCriterion)
{
	paramVec(0) = 1.0 / mean(interArrival);
	paramVec(1) = 0;

	//information criterion
	double aic = informationCriterion(interArrival, "exponential", paramVec,
			"aic");
	double bic = informationCriterion(interArrival, "exponential", paramVec,
			"bic");
	infoCriterion(0) = aic;
	infoCriterion(1) = bic;
}

inline void DataProcessor::paretoLrFitting(const vec& interArrival,
		const vec& interArrivalCdf, vec& paramVec, vec& infoCriterion)
{
	//init
	int iterations = 1500;
	double learning_rate = 0.01;
	vec theta = zeros(2);
	vec J_history = zeros(iterations);

	//linearization
	vec y = log(1.00 + diferential - interArrivalCdf);
	//vec y = log(1.00 - interArrivalCdf);
	vec x = log(interArrival);
	mat* X = featureMatrix(x);

	//gradient descendent
	gradientDescendent(*X, y, learning_rate, iterations, theta, J_history);

	//parameter evaluation
	double pareto_alpha = -theta(1);
	double pareto_xm = interArrival.min();
	paramVec(0) = pareto_alpha;
	paramVec(1) = pareto_xm;

	delete X;

	//information criterion
	double aic = informationCriterion(interArrival, "pareto", paramVec, "aic");
	double bic = informationCriterion(interArrival, "pareto", paramVec, "bic");
	infoCriterion(0) = aic;
	infoCriterion(1) = bic;
}

inline void DataProcessor::paretoMlhFitting(const vec& interArrival,
		const vec& interArrivalCdf, vec& paramVec, vec& infoCriterion)
{
	double pareto_xm = interArrival.min();
	int m = interArrival.size();
	//vec logDiff = log(interArrival) - log(ones<vec>(m) * pareto_xm);
	vec logDiff = log(interArrival) - log(pareto_xm);
	double pareto_alpha = double(m) / (sum(logDiff));
	paramVec(0) = pareto_alpha;
	paramVec(1) = pareto_xm;

	//information criterion
	double aic = informationCriterion(interArrival, "pareto", paramVec, "aic");
	double bic = informationCriterion(interArrival, "pareto", paramVec, "bic");
	infoCriterion(0) = aic;
	infoCriterion(1) = bic;
}

inline void DataProcessor::cauchyFitting(const vec& interArrival,
		const vec& interArrivalCdf, vec& paramVec, vec& infoCriterion)
{
	//initialization
	int iterations = 1500;
	double learning_rate = 0.01;
	int m = interArrival.size();
	double minInterArrival = interArrival.min();
	double y_max = 1.0 / minInterArrival;
	vec y = tan(datum::pi * (interArrivalCdf - 0.5));
	for (int i = 0; i < m; i++)
	{
		//avoid too high numbers when tan() tends to infinity
		if (y(i) > y_max)
			y(i) = y_max;
	}

	//linearization
	vec J_history = zeros<vec>(iterations);
	vec theta = zeros(2);
	mat* X = featureMatrix(interArrival);

	//gradient descendent
	gradientDescendent(*X, y, learning_rate, iterations, theta, J_history);

	//parameter evaluation
	double cauchy_gamma = double(1.0) / theta(1);
	double cauchy_x0 = -theta(0) / theta(1);
	paramVec(0) = cauchy_gamma;
	paramVec(1) = cauchy_x0;

	delete X;

	//information criterion
	double aic = informationCriterion(interArrival, "cauchy", paramVec, "aic");
	double bic = informationCriterion(interArrival, "cauchy", paramVec, "bic");
	infoCriterion(0) = aic;
	infoCriterion(1) = bic;

}

void DataProcessor::constantFitting(const vec& interArrival, vec& paramVec,
		vec& informationCriterion)
{
	paramVec(0) = mean(interArrival);
	paramVec(1) = 0;
	informationCriterion(0) = datum::inf;
	informationCriterion(1) = datum::inf;
}

template<typename T> bool DataProcessor::isEqual(const T* vet1, const T* vet2,
		const int size)
{
	for (int i = 0; i < size; i++)
	{
		if (vet1[i] != vet2[i])
		{
			return (false);
		}
	}

	return (true);
}

template<typename T> void DataProcessor::vectorC_to_list(list<T>* theList,
		T* vet, int size)
{
	for (int i = 0; i < size; i++)
	{
		theList->push_back(vet[i]);
	}
}

template<typename T>
void DataProcessor::quickSort(T* vet, int left, int right)
{
	int p = left;
	int i = 0;
	T ch = 0;
	int j = 0;

	for (i = left + 1; i <= right; i++)
	{
		j = i;
		if (vet[j] < vet[p])
		{
			ch = vet[j];
			while (j > p)
			{
				vet[j] = vet[j - 1];
				j--;
			}
			vet[j] = ch;
			p++;
		}
	}
	if (p - 1 >= left)
		quickSort(vet, left, p - 1);
	if (p + 1 <= right)
		quickSort(vet, p + 1, right);
}

template<typename T>
T DataProcessor::mode(list<T>* theList)
{
	T* vet;
	int listSize = theList->size();
	int i = 0;
	T candidate = 0;
	T prev = 0;
	int counter = 0;
	int largerCounter = 0;
	T mode = 0;
	vet = new T[listSize];

	if (listSize == 0)
	{
		return (0);
	}

	i = 0;

	for (typename list<T>::iterator it = theList->begin(); it != theList->end();
			it++)
	{
		vet[i] = *it;
		i++;
	}

	quickSort(vet, 0, listSize - 1);

	prev = vet[0];
	largerCounter = 0;
	for (i = 0; i < listSize; i++)
	{
		candidate = vet[i];
		if (candidate == prev)
		{
			counter++;
			if (counter > largerCounter)
			{
				largerCounter = counter;
				mode = vet[i];
			}
		}
		else
			counter = 1;

		prev = vet[i];
	}

	delete[] vet;

	return (mode);
}

template<typename T>
inline T* DataProcessor::list_to_cvector(list<T>* theList)
{
	int listSize = theList->size();
	T* vet;
	int i = 0;

	vet = new T[listSize];
	if (listSize == 0)
	{
		return (0);
	}

	i = 0;
	for (typename list<T>::iterator it = theList->begin(); it != theList->end();
			it++)
	{
		vet[i] = *it;
		i++;
	}

	return (vet);
}
template<typename T>
inline void DataProcessor::delete_cvector(T* c_vet)
{
	delete[] c_vet;
}

inline vec* DataProcessor::empiricalCdf(list<double>& empiricalData)
{
	double* cvet_empiricalData = list_to_cvector(&empiricalData);
	int data_size = empiricalData.size();
	quickSort(cvet_empiricalData, 0, data_size - 1);
	double x = cvet_empiricalData[0]; // anchor - fist element;
	int i1 = 0; // fist index
	int i2 = 0;
	double prob = 0;
	vec* interArrivalCdf = new vec(zeros<vec>(data_size));

	for (int i = 0; i < data_size; i++)
	{
		if (cvet_empiricalData[i] > x)
		{
			i2 = i - 1;
			prob = double(i) / data_size;
			for (int j = i1; j <= i2; j++)
			{
				(*interArrivalCdf)(j) = prob;
			}
			i1 = i;
			x = cvet_empiricalData[i];
		}
	}
	//last position == 1
	(*interArrivalCdf)(data_size - 1) = 1;

	delete_cvector(cvet_empiricalData);
	return (interArrivalCdf);
}

inline mat* DataProcessor::featureMatrix(list<double>& empiricalData)
{
	int data_size = empiricalData.size();
	mat* featureMatrix = new mat(ones<mat>(data_size, 2));

	int i = 0;
	for (list<double>::iterator it = empiricalData.begin();
			it != empiricalData.end(); it++)
	{
		(*featureMatrix)(i, 1) = *it;
		i++;
	}

	return (featureMatrix);

}

inline mat* DataProcessor::featureMatrix(const vec& empiricalData)
{
	int data_size = empiricalData.size();
	mat* featureMatrix = new mat(ones<mat>(data_size, 2));
	for (int i = 0; i < data_size; i++)
	{
		(*featureMatrix)(i, 1) = empiricalData(i);
	}

	return (featureMatrix);
}

inline double DataProcessor::computeCost(const mat& X, const vec& y,
		const vec& theta)
{
	double J = .0;
	double jtemp = .0;
	int m = y.size();

	for (int i = 0; i < m; i++)
	{
		jtemp = jtemp + pow(double(theta(0) + theta(1) * X(i, 1) - y(i)), 2);

	}
	J = (1.0 / double(2 * m)) * jtemp;

	return (J);
}

inline void DataProcessor::gradientDescendent(const mat& X, const vec& y,
		const double learning_rate, const int num_iters, vec& theta,
		vec& J_history)
{

	int m = y.n_rows; //number of training examples
	double temp1 = .0;
	double temp2 = .0;
	double delta1 = .0;
	double delta2 = .0;
	vec x = X.col(1);

	for (int i = 0; i < num_iters; i++)
	{
		temp1 = .0;
		temp2 = .0;
		delta1 = .0;
		delta2 = .0;

		//slope
		delta1 = sum(theta(0) + theta(1) * x - y) / double(m);
		delta2 = sum((theta(0) + theta(1) * x - y) % x) / double(m);

		//linear regression
		temp1 = theta(0) - learning_rate * delta1;
		temp2 = theta(1) - learning_rate * delta2;

		theta(0) = temp1;
		theta(1) = temp2;

		//Save the cost J in every iteration
		J_history(i) = computeCost(X, y, theta);

	}

}

inline double DataProcessor::informationCriterion(const vec& data,
		const string& functionName, const vec& paramVet,
		const string& criterion)
{
	double criterionVal = .0;
	double nEstimatedParameters = double(paramVet.size());
	double likehoodLogVal = logLikehood(data, functionName, paramVet);
	double logDataLen = log(double(data.size()));

	if (criterion == "aic")
	{
		criterionVal = 2 * nEstimatedParameters - 2 * likehoodLogVal;
	}
	else if (criterion == "bic")
	{
		criterionVal = nEstimatedParameters * logDataLen - 2 * likehoodLogVal;
	}
	else
	{
		cout << "\nInvalid functionName or no functionName selected: "
				<< functionName << endl;
		printf("Selecting default criterion: AIC\n");
		criterionVal = 2 * nEstimatedParameters - 2 * likehoodLogVal;
	}

	return (criterionVal);

}

inline double DataProcessor::logLikehood(const vec& data,
		const string& functionName, const vec& paramVet)
{
	double param1 = paramVet(0);
	double param2 = paramVet(1);
	double m = data.size();
	double L = .0;
	int i = 0;
	vec data_prob = zeros<vec>(size(data));

	if (functionName == "weibull")
	{
		for (i = 0; i < m; i++)
			data_prob(i) = pdf_weibull(data(i), param1, param2);
	}
	else if (functionName == "normal")
	{
		for (i = 0; i < m; i++)
			data_prob(i) = pdf_normal(data(i), param1, param2);
	}
	else if (functionName == "uniform")
	{
		data_prob = ones<vec>(size(data)) * pdf_uniform(param1, param2);
	}
	else if (functionName == "exponential")
	{
		for (i = 0; i < m; i++)
			data_prob(i) = pdf_exponential(data(i), param1);
	}
	else if (functionName == "pareto")
	{
		for (i = 0; i < m; i++)
			data_prob(i) = pdf_pareto(data(i), param1, param2);
	}
	else if (functionName == "cauchy")
	{
		for (i = 0; i < m; i++)
			data_prob(i) = pdf_cauchy(data(i), param1, param2);
	}
	else
	{
		cout << "Warning, no valid stochastic function selected: "
				<< functionName << endl;
		printf("Likehood logarithm seted to -Inf (worst as possible)\n");
		return (datum::inf);
	}

	L = sum(log(data_prob));
	return (L);

}

inline double DataProcessor::pdf_weibull(double x, double alpha, double betha)
{
	if (x >= 0)
		return ((alpha / pow(betha, alpha)) * pow(x, alpha - 1.0)
				* exp(-(pow(x / betha, alpha))));
	else
		return (.0);
}

inline double DataProcessor::cdf_weibull(double x, double alpha, double betha)
{
	if (x >= 0)
		return (1.0 - exp(-pow(x / betha, alpha)));
	else
		return (.0);
}

inline double DataProcessor::pdf_exponential(double x, double lambda)
{
	if (x >= 0)
		return (lambda * exp(-lambda * x));
	else
		return (.0);

}

inline double DataProcessor::cdf_exponential(double x, double lambda)
{
	if (x >= 0)
		return (1.0 - exp(-lambda * x));
	else
		return (.0);
}

inline double DataProcessor::pdf_pareto(double x, double alpha, double xm)
{
	if (x >= xm)
		return (alpha * pow(xm, alpha) / (pow(x, alpha + 1.0)));
	else
		return (.0);
}

inline double DataProcessor::cdf_pareto(double x, double alpha, double xm)
{
	if (x >= xm)
		return (1.0 - pow(xm / x, alpha));
	else
		return (.0);
}

inline double DataProcessor::pdf_cauchy(double x, double gamma, double x0)
{
	return (1 / (datum::pi * gamma)
			* (pow(gamma, 2) / (pow(x - x0, 2) + pow(gamma, 2))));
}

inline double DataProcessor::cdf_cauchy(double x, double gamma, double x0)
{
	return ((1 / datum::pi) * atan((x - x0) / (gamma)) + 0.5);
}

inline double DataProcessor::pdf_normal(double x, double mu, double sigma)
{
	return (1 / (sqrt(2 * datum::pi) * sigma)
			* exp(-0.5 * pow((x - mu) / sigma, 2)));
}

inline double DataProcessor::cdf_normal(double x, double mu, double sigma)
{
	return (0.5 * (1 + erf((x - mu) / (sigma * sqrt(2)))));
}

inline double DataProcessor::pdf_uniform(double x_min, double x_max)
{
	return (1.0 / (x_max - x_min));
}

inline double DataProcessor::cdf_uniform(double x, double x_min, double x_max)
{
	if (x < x_min)
		return (0);
	else if (x > x_max)
		return (1);
	else
		return ((x - x_min) / (x_max - x_min));
}

#ifdef TEST_FUNCTIONS

void DataProcessor::printTestResult(string testName, bool result)
{
	string outRes;
	if (result == true)
		outRes = "ok";
	else
		outRes = "failed";

	cout << testName << ":\t\t" << outRes << endl;
}

inline bool DataProcessor::compareDouble(double val1, double val2)
{
	double error = 0;

	error = fabs(double(val1) - double(val2));
	if (error > double(1e-5))
		return (false);
	else
		return (true);

}

inline bool DataProcessor::compareDouble(double val1, double val2, double acErr)
{
	double error = 0;

	error = fabs(double(val1) - double(val2));
	if (error > double(acErr))
		return (false);
	else
		return (true);

}

bool DataProcessor::compareMat(mat& mat1, mat& mat2)
{
	int mat1_nc = mat1.n_cols;
	int mat1_nr = mat1.n_rows;
	int mat2_nc = mat2.n_cols;
	int mat2_nr = mat2.n_rows;

	if ((mat1_nc != mat2_nc) || (mat1_nr != mat2_nr))
	{
		cout << "mat1.n_cols and mat2.n_cols dont match. " << mat1_nc << ":"
				<< mat2_nc << " " << mat1_nr << ":" << mat2_nr << endl;
		return (false);
	}
	for (int i = 0; i < mat1_nr; i++)
	{
		for (int j = 0; j < mat1_nc; j++)
		{
			if (compareDouble(mat1(i, j), mat2(i, j), 1e-5))
			{
				//cout << " mat1(" << i << ", " << j << " ) = " << mat1(i,j);
				//cout << " mat2(" << i << ", " << j << " ) = " << mat2(i,j);
				return (false);
			}
		}
	}

	return (true);
}

inline void DataProcessor::save_data_on_file(const string& fileName,
		const mat& vet1, const mat& vet2)
{
	string file_out = fileName + ".txt";
	int m = vet1.size();

	ofstream writeOnFile(file_out, ios::out);
	if (!writeOnFile)
	{
		cerr << "file could not be oppeded" << endl;
		exit(1);
	}

	for (int i = 0; i < m; i++)
	{
		writeOnFile << vet1(i) << " " << vet2(i) << endl;
	}

}

bool DataProcessor::testMode()
{

	int randint1[] =
	{ 1, 8, 1, 13, 9, 2, 1, 15, 4, 3, 10, 4, 13, 2, 14, 7, 7 };
	int randint1_mode = 1;
	list<int> l_randint1;
	vectorC_to_list(&l_randint1, randint1, 17);

	int randint2[] =
	{ 5, 3, 6, 8, 14, 9, 11, 9, 11, 15, 8, 7, 12, 13, 14 };
	int randint2_mode = 8;
	list<int> l_randint2;
	vectorC_to_list(&l_randint2, randint2, 15);

	int randint3[] =
	{ 9, 10, 6, 3, 13, 13, 14, 7, 7, 1, 5, 1, 8, 5, 9, 2, 3, 4, 5, 6, 6, 6 };
	int randint3_mode = 6;
	list<int> l_randint3;
	vectorC_to_list(&l_randint3, randint3, 22);

	int randint4[] =
	{ 2, 1 };
	int randint4_mode = 1;
	list<int> l_randint4;
	vectorC_to_list(&l_randint4, randint4, 2);

	int randint5[] =
	{ 1, 1, 1, 1, 1, 2, 2, 2, 2, 2 };
	int randint5_mode = 1;
	list<int> l_randint5;
	vectorC_to_list(&l_randint5, randint5, 10);

	if (mode(&l_randint1) != randint1_mode)
		return (false);
	if (mode(&l_randint2) != randint2_mode)
		return (false);
	if (mode(&l_randint3) != randint3_mode)
		return (false);
	if (mode(&l_randint4) != randint4_mode)
		return (false);
	if (mode(&l_randint5) != randint5_mode)
		return (false);

	return (true);
}

bool DataProcessor::testQuickSort()
{
	int randIntA[] =
	{ 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9 };
	int randIntA_size = 18;
	int randIntA_sorted[] =
	{ 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9 };
	int randIntB[] =
	{ 9, 2, 3, 3, 12, 15, 8, 14, 2, 7, 4, 6, 10, 2, 12 };
	int randIntB_size = 15;
	int randIntB_sorted[] =
	{ 2, 2, 2, 3, 3, 4, 6, 7, 8, 9, 10, 12, 12, 14, 15 };
	double randDoubleA[] =
	{ 0.20872, 12.25501, 5.88210, 2.77962, 8.74916, 2.97454, 4.39641, 12.19733,
			4.66014, 4.52477, 5.19375, 12.11314, 13.35458, 11.56941, 6.59435 };
	int randDoubleA_size = 15;
	double randDoubleA_sorted[] =
	{ 0.20872, 2.77962, 2.97454, 4.39641, 4.52477, 4.66014, 5.19375, 5.88210,
			6.59435, 8.74916, 11.56941, 12.11314, 12.19733, 12.25501, 13.35458 };

	quickSort(randIntA, 0, randIntA_size - 1);

	if (isEqual(randIntA, randIntA_sorted, randIntA_size) == false)
	{
		return (false);
	}

	quickSort(randIntB, 0, randIntB_size - 1);
	if (isEqual(randIntB, randIntB_sorted, randIntB_size) == false)
	{
		return (false);
	}

	quickSort(randDoubleA, 0, randDoubleA_size - 1);
	if (isEqual(randDoubleA, randDoubleA_sorted, randDoubleA_size) == false)
	{
		return (false);
	}

	return (true);

}

inline bool DataProcessor::test_list_tocvector()
{
	list<int> randInt_list1 =
	{ 9, 2, 3, 3, 12, 15, 8, 14, 2, 7, 4, 6, 10, 2, 12 };
	list<double> randInt_list2 =
	{ 2.2, 2.111, 3.16, 6.66, 13, 2.61 };
	list<int> randInt_list3 =
	{ 1, 2, 3, 4, 5, 6, 6, 5, 4, 3, 2, 1 };

	int* cvet1 = NULL;
	double* cvet2 = NULL;
	int* cvet3 = NULL;

	cvet1 = list_to_cvector(&randInt_list1);
	cvet2 = list_to_cvector(&randInt_list2);
	cvet3 = list_to_cvector(&randInt_list3);

	int i = 0;
	for (list<int>::iterator it = randInt_list1.begin();
			it != randInt_list1.end(); it++)
	{
		if (cvet1[i] != *it)
		{
			return (false);
		}
		//cout << *it << " ";
		i++;
	}
	i = 0;
	for (list<double>::iterator it = randInt_list2.begin();
			it != randInt_list2.end(); it++)
	{
		if (cvet2[i] != *it)
		{
			return (false);
		}
		//cout << *it << " ";
		i++;
	}
	i = 0;
	for (list<int>::iterator it = randInt_list3.begin();
			it != randInt_list3.end(); it++)
	{
		if (cvet3[i] != *it)
		{
			return (false);
		}
		//cout << *it << " ";
		i++;
	}

	delete_cvector(cvet1);
	delete_cvector(cvet2);
	delete_cvector(cvet3);

	return (true);
}

inline bool DataProcessor::test_empiricalCdf()
{
	list<double> interArrival_sample =
	{ 5.0000e-07, 1.5000e-06, 1.5000e-06, 2.5000e-06, 2.5000e-06, 3.5000e-06,
			4.5000e-06, 7.5000e-06, 1.2500e-05, 1.9500e-05, 3.1500e-05,
			4.3500e-05, 6.0500e-05, 7.6500e-05, 1.1050e-04, 1.4050e-04,
			1.8150e-04, 2.1150e-04, 2.3350e-04, 2.4350e-04, 2.5650e-04,
			3.0350e-04, 4.1250e-04, 1.3655e-03, 1.1593e-02, 3.2483e-02,
			8.1235e-02, 2.0939e-01, 5.0755e-01, 1.7351e+00 };
	vec emCdf_octave =
	{ 0.033333, 0.100000, 0.100000, 0.166667, 0.166667, 0.200000, 0.233333,
			0.266667, 0.300000, 0.333333, 0.366667, 0.400000, 0.433333,
			0.466667, 0.500000, 0.533333, 0.566667, 0.600000, 0.633333,
			0.666667, 0.700000, 0.733333, 0.766667, 0.800000, 0.833333,
			0.866667, 0.900000, 0.933333, 0.966667, 1.000000 };
	//emCdf_octave = emCdf_octave.t();

	vec* emCdf_cpp = empiricalCdf(interArrival_sample);

	int list_size = interArrival_sample.size();
	for (int i = 0; i < list_size; i++)
	{
		if (!compareDouble(emCdf_octave(i), (*emCdf_cpp)(i), 1e-6))
		{
			cout << "cdf error @ " << "i:" << i << " res:" << (*emCdf_cpp)(i, 0)
					<< " expected:" << emCdf_octave(i, 0) << endl;
			return (false);
		}
	}

	delete emCdf_cpp;
	return (true);
}

inline bool DataProcessor::test_computeCost()
{
	list<double> interArrival_sample =
	{ 5.0000e-07, 1.5000e-06, 1.5000e-06, 2.5000e-06, 2.5000e-06, 3.5000e-06,
			4.5000e-06, 7.5000e-06, 1.2500e-05, 1.9500e-05, 3.1500e-05,
			4.3500e-05, 6.0500e-05, 7.6500e-05, 1.1050e-04, 1.4050e-04,
			1.8150e-04, 2.1150e-04, 2.3350e-04, 2.4350e-04, 2.5650e-04,
			3.0350e-04, 4.1250e-04, 1.3655e-03, 1.1593e-02, 3.2483e-02,
			8.1235e-02, 2.0939e-01, 5.0755e-01, 1.7351e+00 };
	mat* X = featureMatrix(interArrival_sample);
	vec emCdf_octave =
	{ 0.033333, 0.100000, 0.100000, 0.166667, 0.166667, 0.200000, 0.233333,
			0.266667, 0.300000, 0.333333, 0.366667, 0.400000, 0.433333,
			0.466667, 0.500000, 0.533333, 0.566667, 0.600000, 0.633333,
			0.666667, 0.700000, 0.733333, 0.766667, 0.800000, 0.833333,
			0.866667, 0.900000, 0.933333, 0.966667, 1.000000 };
	//emCdf_octave = emCdf_octave.t();
	double J_expected1 = 0.17535;
	double J_expected2 = 1.5341;
	double J_expected3 = 9.0244;

	vec theta = zeros(2);

	double J = computeCost(*X, emCdf_octave, theta);
	if (!compareDouble(J, J_expected1, 0.001))
	{
		//cout << "J1 = " << J << "\nJ_expected1 = "<< J_expected1 << endl;
		return (false);
	}
	theta(0) = 2;
	theta(1) = 2;
	J = computeCost(*X, emCdf_octave, theta);
	if (!compareDouble(J, J_expected2, 0.001))
	{
		//cout << "J2 = " << J << "\nJ_expected2 = "<< J_expected2 << endl;
		return (false);
	}
	theta(0) = 1.5;
	theta(1) = 12;
	J = computeCost(*X, emCdf_octave, theta);
	if (!compareDouble(J, J_expected3, 0.001))
	{
		//cout << "J3 = " << J << "\nJ_expected3 = "<< J_expected3 << endl;
		return (false);
	}

	delete X;
	return (true);
}

inline bool DataProcessor::test_informationCriterion()
{
	//mat M;
	//M.load("CppAlgorithms/data_sample.txt");
	//vec interArrival = sort(M.col(3)) + min_time;
	vec interArrival = sort(interArrivalSample) + min_time;

	int m = interArrival.size();
	list<double> list_interArrival;
	vec* interArrivalCdf;
	for (int i = 0; i < m; i++)
	{
		list_interArrival.push_back(interArrival(i));
	}
	interArrivalCdf = empiricalCdf(list_interArrival);

	//gradient descendent
	int iterations = 1500;
	double learning_rate = 0.01;
	vec theta = zeros(2);
	vec J_history = zeros(iterations);

	vec y = log(-log(1.0 + diferential - *interArrivalCdf));
	vec x = log(interArrival);
	mat* X = featureMatrix(x);

	gradientDescendent(*X, y, learning_rate, iterations, theta, J_history);

	double weibull_alpha = theta(1);
	double weibull_betha = exp(-theta(0) / theta(1));
	vec paramVec =
	{ weibull_alpha, weibull_betha };
	double weibull_bic = informationCriterion(interArrival, "weibull", paramVec,
			"bic");
	double weibull_aic = informationCriterion(interArrival, "weibull", paramVec,
			"aic");

	if (!compareDouble(weibull_bic, -33712.7042, 0.001))
		return (false);
	else if (!compareDouble(weibull_aic, -33724.6567, 0.001))
		return (false);

	return (true);
}

inline bool DataProcessor::test_pdf_weibull()
{
	double alpha = 1.5000;
	double betha = 0.3;
	vec data =
	{ 0.010000, 2.300000, 0.500000 };
	vec dataprob_eval1 =
	{ 9.0733e-01, 8.3571e-09, 7.5066e-01 };
	vec dataprob_test = zeros(3);

	dataprob_test(0) = pdf_weibull(data(0), alpha, betha);
	dataprob_test(1) = pdf_weibull(data(1), alpha, betha);
	dataprob_test(2) = pdf_weibull(data(2), alpha, betha);

	if (compareDouble(dataprob_test(0), dataprob_eval1(0))
			&& compareDouble(dataprob_test(1), dataprob_eval1(1))
			&& compareDouble(dataprob_test(2), dataprob_eval1(2)))
	{
		return (true);
	}

	return (false);
}

inline bool DataProcessor::test_cdf_weibull()
{
	double alpha = 0.5;
	double betha = 1.5;
	vec data =
	{ 0.010000, 2.300000, 0.500000 };
	vec dataprob_eval1 =
	{ 0.078405, 0.710117, 0.438616 };
	vec dataprob_test = zeros(3);

	dataprob_test(0) = cdf_weibull(data(0), alpha, betha);
	dataprob_test(1) = cdf_weibull(data(1), alpha, betha);
	dataprob_test(2) = cdf_weibull(data(2), alpha, betha);
	//cout << dataprob_test(0) << " : " << dataprob_test(1) << " : " << dataprob_test(2) << endl;

	if (compareDouble(dataprob_test(0), dataprob_eval1(0))
			&& compareDouble(dataprob_test(1), dataprob_eval1(1))
			&& compareDouble(dataprob_test(2), dataprob_eval1(2)))
	{
		return (true);
	}

	return (false);
}

inline bool DataProcessor::test_gradientDescendent()
{
	//mat M;
	//M.load("CppAlgorithms/data_sample.txt");
	//vec interArrival = sort(M.col(3)) + min_time;
	vec interArrival = sort(interArrivalSample) + min_time;

	int m = interArrival.size();
	list<double> list_interArrival;
	vec* interArrivalCdf;
	for (int i = 0; i < m; i++)
	{
		list_interArrival.push_back(interArrival(i));
	}
	interArrivalCdf = empiricalCdf(list_interArrival);

	//gradient descendent
	int iterations = 1500;
	double learning_rate = 0.01;
	vec theta = zeros(2);
	vec J_history = zeros(iterations);

	vec y = log(-log(1.0 + diferential - *interArrivalCdf));
	vec x = log(interArrival);
	mat* X = featureMatrix(x);

	gradientDescendent(*X, y, learning_rate, iterations, theta, J_history);

	//expected values
	double Jlast = 0.083359;
	double theta0 = 1.660453;
	double theta1 = 0.248811;

	if (!compareDouble(J_history(iterations - 1), Jlast))

	{
		return (false);
	}
	else if (!compareDouble(theta(0), theta0))
	{
		return (false);
	}
	else if (!compareDouble(theta(1), theta1))
	{
		return (false);
	}

	delete interArrivalCdf;
	delete X;
	return (true);

}

inline bool DataProcessor::test_pdf_exponential()
{
	vec data =
	{ 0.010000, 2.300000, 0.500000 };

	double lambda1 = 0.50000;
	vec val1 =
	{ 0.49751, 0.15832, 0.38940 };
	vec res1 = zeros<vec>(3);

	double lambda2 = 3.0;
	vec val2 =
	{ 2.9113366, 0.0030234, 0.6693905 };
	vec res2 = zeros<vec>(3);

	for (int i = 0; i < 3; i++)
	{
		//exponential
		res1(i) = pdf_exponential(data(i), lambda1);
		res2(i) = pdf_exponential(data(i), lambda2);

		if (!compareDouble(res1(i), val1(i)))
		{
			return (false);
		}
		if (!compareDouble(res2(i), val2(i)))
		{
			return (false);
		}

	}

	return (true);
}

inline bool DataProcessor::test_cdf_exponential()
{
	vec data =
	{ 0.010000, 2.300000, 0.500000 };

	double lambda1 = 0.50000;
	vec val1 =
	{ 0.0049875, 0.6833632, 0.2211992 };
	vec res1 = zeros<vec>(3);

	double lambda2 = 3.0;
	vec val2 =
	{ 0.029554, 0.998992, 0.776870 };
	vec res2 = zeros<vec>(3);

	for (int i = 0; i < 3; i++)
	{
		//exponential
		res1(i) = cdf_exponential(data(i), lambda1);
		res2(i) = cdf_exponential(data(i), lambda2);

		if (!compareDouble(res1(i), val1(i)))
		{
			return (false);
		}
		if (!compareDouble(res2(i), val2(i)))
		{
			return (false);
		}
	}

	return (true);
}

inline bool DataProcessor::test_pdf_pareto()
{

	vec data =
	{ 0.010000, 2.300000, 0.500000 };

	double alpha1 = 0.50000;
	double xm1 = 1.0;
	vec val1 =
	{ 0, 0.14334, 0 };
	vec res1 = zeros<vec>(3);
	double xm2 = 0.00001;
	double alpha2 = 3.0;
	vec val2 =
	{ 3.0000e-07, 1.0720e-16, 4.8000e-14 };
	vec res2 = zeros<vec>(3);

	for (int i = 0; i < 3; i++)
	{
		//exponential
		res1(i) = pdf_pareto(data(i), alpha1, xm1);
		res2(i) = pdf_pareto(data(i), alpha2, xm2);

		//cout << "res1(i) " << res1(i) << ", res2(i) " << res2(i) << endl;
		if (!compareDouble(res1(i), val1(i)))
		{
			return (false);
		}
		if (!compareDouble(res2(i), val2(i)))
		{
			return (false);
		}
	}

	return (true);
}

inline bool DataProcessor::test_cdf_pareto()
{

	vec data =
	{ 0.010000, 2.300000, 0.500000 };

	double alpha1 = 0.50000;
	double xm1 = 0.001;
	vec val1 =
	{ 0.68377, 0.97915, 0.95528 };
	vec res1 = zeros<vec>(3);
	double xm2 = 0.2;
	double alpha2 = 1;
	vec val2 =
	{ 0, 0.91304, 0.60000 };
	vec res2 = zeros<vec>(3);

	for (int i = 0; i < 3; i++)
	{
		//exponential
		res1(i) = cdf_pareto(data(i), alpha1, xm1);
		res2(i) = cdf_pareto(data(i), alpha2, xm2);

		//cout << "res1(i) " << res1(i) << ", res2(i) " << res2(i) << endl;
		if (!compareDouble(res1(i), val1(i)))
		{
			return (false);
		}
		if (!compareDouble(res2(i), val2(i)))
		{
			return (false);
		}
	}
	return (true);
}

inline bool DataProcessor::test_pdf_cauchy()
{

	vec data =
	{ 0.010000, 2.300000, 0.500000 };
	double gamma1 = 0.50000;
	double x01 = 0.001;

	vec val1 =
	{ 0.636414, 0.028752, 0.318947 };
	vec res1 = zeros<vec>(3);
	double x02 = 2;
	double gamma2 = 2;

	vec val2 =
	{ 0.079976, 0.155653, 0.101859 };
	vec res2 = zeros<vec>(3);

	for (int i = 0; i < 3; i++)
	{
		//exponential
		res1(i) = pdf_cauchy(data(i), gamma1, x01);
		res2(i) = pdf_cauchy(data(i), gamma2, x02);

		//cout << "res1(i) " << res1(i) << ", res2(i) " << res2(i) << endl;
		if (!compareDouble(res1(i), val1(i)))
		{
			return (false);
		}
		if (!compareDouble(res2(i), val2(i)))
		{
			return (false);
		}
	}
	return (true);
}

inline bool DataProcessor::test_cdf_cauchy()
{

	vec data =
	{ 0.010000, 2.300000, 0.500000 };
	double gamma1 = 0.50000;
	double x01 = 0.001;

	vec val1 =
	{ 0.50573, 0.93183, 0.74968 };
	vec res1 = zeros<vec>(3);
	double x02 = 2;
	double gamma2 = 2;

	vec val2 =
	{ 0.25080, 0.54739, 0.29517 };
	vec res2 = zeros<vec>(3);

	for (int i = 0; i < 3; i++)
	{
		//exponential
		res1(i) = cdf_cauchy(data(i), gamma1, x01);
		res2(i) = cdf_cauchy(data(i), gamma2, x02);

		//cout << "res1(i) " << res1(i) << ", res2(i) " << res2(i) << endl;
		if (!compareDouble(res1(i), val1(i)))
		{
			return (false);
		}
		if (!compareDouble(res2(i), val2(i)))
		{
			return (false);
		}
	}
	return (true);
}

inline bool DataProcessor::test_fitModels()
{
	//mat M;
	//M.load("CppAlgorithms/data_sample.txt");
	//vec interArrival = sort(M.col(3)) + min_time;
	vec interArrival = sort(interArrivalSample) + min_time;

	int m = interArrival.size();
	list<double> list_interArrival;
	vec* interArrivalCdf;
	for (int i = 0; i < m; i++)
	{
		list_interArrival.push_back(interArrival(i));
	}
	interArrivalCdf = empiricalCdf(list_interArrival);

	//list of expected values
	double weibull_alpha = 0.24881;
	double weibull_betha = 0.0012639;
	double normal_mu = 0.054792;
	double normal_sigma = 0.205602;
	double exponentialMe_lambda = 18.250783;
	double exponentialLr_lambda = 2.023115;
	double paretoLr_alpha = 0.225451;
	double paretoLr_xm = 0.000000;
	double paretoMlh_alpha = 0.171674;
	double paretoMlh_xm = 0.000000;
	double cauchy_gamma = 0.000067;
	double cauchy_x0 = 0.005750;
	double acErr = 0.001;

	vec paramVec = zeros<vec>(2);
	vec infoCriterion = zeros<vec>(2);

	weibullFitting(interArrival, *interArrivalCdf, paramVec, infoCriterion);
	if (!compareDouble(paramVec(0), weibull_alpha, acErr))
	{
		cout << "weibullFitting alpha" << endl;
		return (false);
	}
	if (!compareDouble(paramVec(1), weibull_betha, acErr))
	{
		cout << "weibullFitting betha" << endl;
		return (false);
	}

	normalFitting(interArrival, paramVec, infoCriterion);
	if (!compareDouble(paramVec(0), normal_mu, acErr))
	{
		cout << "normalFitting mu" << endl;
		return (false);
	}
	if (!compareDouble(paramVec(1), normal_sigma, acErr))
	{
		cout << "normalFitting sigma" << endl;
		return (false);
	}

	exponentialMeFitting(interArrival, paramVec, infoCriterion);
	if (!compareDouble(paramVec(0), exponentialMe_lambda, acErr))
	{
		cout << "exponentialLrFitting lambda" << endl;
		return (false);
	}

	exponentialLrFitting(interArrival, *interArrivalCdf, paramVec,
			infoCriterion);
	if (!compareDouble(paramVec(0), exponentialLr_lambda, acErr))
	{
		cout << exponentialLr_lambda << ":" << paramVec(0) << endl;
		cout << "exponentialLrFitting lambda" << endl;
		return (false);
	}

	paretoLrFitting(interArrival, *interArrivalCdf, paramVec, infoCriterion);
	if (!compareDouble(paramVec(0), paretoLr_alpha, acErr))
	{
		cout << "paretoLrFitting alpha" << endl;
		return (false);
	}
	if (!compareDouble(paramVec(1), paretoLr_xm, acErr))
	{
		cout << "paretoLrFitting xm" << endl;
		return (false);
	}

	paretoMlhFitting(interArrival, *interArrivalCdf, paramVec, infoCriterion);
	if (!compareDouble(paramVec(0), paretoMlh_alpha, acErr))
	{
		cout << "paretoMlhFitting alpha" << endl;
		return (false);
	}
	if (!compareDouble(paramVec(1), paretoMlh_xm, acErr))
	{
		cout << "paretoMlhFitting xm" << endl;
		return (false);
	}

	cauchyFitting(interArrival, *interArrivalCdf, paramVec, infoCriterion);
	if (!compareDouble(paramVec(0), cauchy_gamma, acErr))
	{
		cout << "cauchyFitting" << endl;
		return (false);
	}
	if (!compareDouble(paramVec(1), cauchy_x0, acErr))
	{
		return (false);
	}

	delete interArrivalCdf;

	return (true);
}

inline bool DataProcessor::test_modelSelection()
{

	//mat M;
	//M.load("CppAlgorithms/data_sample.txt");
	//vec interArrival = sort(M.col(3)) + min_time;
	vec interArrival = sort(interArrivalSample) + min_time;
	int m = interArrival.size();

	list<double> list_interArrival;
	vec* interArrivalCdf;
	for (int i = 0; i < m; i++)
	{
		list_interArrival.push_back(interArrival(i));
	}
	interArrivalCdf = empiricalCdf(list_interArrival);

	StochasticModelFit* modelVet = NULL;
	modelVet = fitModels(list_interArrival, "aic");
	//fitModels(list_interArrival, "aic", modelVet);

	int numberOfModels = modelVet[0].size;
	for (int i = 0; i < numberOfModels; i++){
		cout << modelVet[i].modelName << " " << "modelVet[" << i << "] = "
				<< modelVet[i].aic << " param1="<< modelVet[i].param1 << " param2="<< modelVet[i].param2 << endl;
	}

	delete interArrivalCdf;
	delete[] modelVet;
	return (true);
}
#endif

