\chapter{Modeling and Algorithms}\label{ch:modeling-evaluation}

\section{Introduction}

As discussed before, there is many works devoted on studding the nature of the Ethernet traffic \cite{selfsimilar-ethernet}. Classic Ethernet models used Poisson related process to express generation of traffic. In a first look it makes sense,  sense, since a Poisson process represents the probability of events occur with a known average rate, and independently of the time since the last event\cite{selfsimilar-ethernet} \cite{book-poisson}. But studies made by Leland et al.\cite{selfsimilar-ethernet} showed that the Ethernet traffic has a self-similar and fractal nature. Even if they are able to represent the randomness of an Ethernet traffic, simple Poisson processes can't express the presence of "burstiness" in a long-term time scale, such as traffic "spikes" on long-range "ripples".  This is an indicative of the fractal and self-similar nature of the traffic.

Stochastic process that respects this characteristic have infinite variance, said to be heavy-tailed. Heavy-tail means that a stochastic distribution is not exponentially bounded\cite{sourcesonoff-paper}. Examples of heavy-tailed functions are Weibull, Pareto, Cauchy. But not many works on the literature deal with a process for parameterization of different stochastic models for Ethernet traffic. Also, a heavy-tailed function may guarantee self-similarity, but not necessarily they would guarantee other features like good correlation and same mean packet rate and dispersion.

In this chapter we proposed an unified and simple method for modeling inter packet times, and choose the best fit automatically, without having to generate random data. We estimate many stochastic functions through many methodologies, and select the best model through the AIC computation. Since generating random can be computationally cost an have a bias, depending of the seed generator, an analytical method for selecting the best fitting eliminate these problems.

We made a brief bibliographic review on some works related works. Then, we will present the traffic traces we are going to use in this chapter, and in the rest of the work. After we present or selected methodology for parameterization selection of the best fitting. To evaluate the quality of this selection criteria, we define a methodology for evaluate if our model selection is good or not. 

\section{Related Works}

There are plenty of works on the literature which proposes new processes and methodologies foe generation on inter-packet times.

Fiorini \cite{modeling-concurrent-heavy-tailed} presents a heavy-tailed ON/OFF model, which tries to represent a traffic generated by many sources. The model emulates a multiple source power-tail Markov-Modulated (PT-MMPP) ON/OFF process, where the ON times are power-tail distributed. They achieve analytical performance measurements using Linear Algebra Queueing Theory.

Kreban and Clearwater\cite{hierarchical-dynamics-interarrival-times} presents a model for times between job submissions of multiple users on a super computer. They show in their job that the Weibull probability functions is able to express well small and high values of inter-job submission times. They also tested exponential, lognormal and Pareto(called there power law) distributions. Exponential distribution couldn't  represent long-range values because it fell off too fast and Pareto was too slow. Lognormal fit well small values, but was poor on larger ones.    

Kronewitter\cite{optimal-scheduling-of-heavy-tailed-traffic} presents a models of scheduler of heavy-tailed traffic of multiple sources. On his work, he uses multiple Pareto sources to represent the traffic. To estimate the shape parameter $\alpha$ they use linear regression.





\section{Methodology}

We will start defining the datasets we are going to use in this evaluation, and in the rest of this work as well. We define our criteria of parameterization and model selection, and a methodology for measure the quality of our choices. 


%In this chapter we will evaluate our proposed method for data modeling of inter-packet times.  We will present curves obtained by our prototype (implemented in Gnu Octave\footnote{\href{https://www.gnu.org/software/octave/}{https://www.gnu.org/software/octave/}}) that shows in detail how each step works. After explaining this process, we will present an evaluation method of our results, analyzing to analyze its quality. After this, we give some details of how we convert code from Octave to C++ and its benefits on time execution. All code used in this section is available on our GitHub page.%, on the directory \texttt{Prototypes}. 

\subsection{Datasets}

We start defining also define the \textit{pcaps} datasets we are going to use in the rest of this text. We will use for datasets, and for reproduction purposes, three are public available. 
The first is a light weight Skype capture, found at Wireshark wiki\footnote{https://wiki.wireshark.org/}, and can be found at \href{https://wiki.wireshark.org/SampleCaptures}{https://wiki.wireshark.org/SampleCaptures}. The file name is \texttt{SkypeIRC.cap}. 

The second is a CAIDA\footnote{http://www.caida.org/home/}{http://www.caida.org/home/} capture, and can be found at  \href{https://data.caida.org/datasets/passive-2016/equinix-chicago/20160121-130000.UTC}{https://data.caida.org/datasets/passive-2016/equinix-chicago/20160121-130000.UTC}. Access to this file need login, so you will have to create an account and wait for approval first. The pcap's file name is \texttt{equinix-chicago.dirB.20160121-135641.UTC.anon.pcap.gz}. 

The third we capture in our laboratory LAN, through a period of 24 hours. It was captured in the firewall gateway between our local and external network. Along with other tests, We intend to verify diurnal behavior on it. That means, a high demand of packets during the day, an a small in the night. The fourth is a capture of a busy private network's access point to the Internet, available on-line on TCPreplay\cite{web-tcpreplay} website \footnote{ http://tcpreplay.appneta.com/wiki/captures.html}, and is called \texttt{bigFlows.pcap}. We will refer to these \textit{pcaps} as \textit{skype-pcap}, \textit{wan-pcap}, \textit{lan-diurnal-pcap}, and \textit{bigflows-pcap} respectivaly. Along with these datasets, we will use two subpcaps from the originals. The first, \textit{skype-flowburst-pcap}, is a burst of a single HTTP flow from \textit{skype-pcap}. It can be obtained from Wireshark using the filter \texttt{(ip.src\_host==172.16.133.25) \&\& (ip.dst\_host==74.125.170.42) \&\& (tcp.dstport==80) \&\& (tcp.srcport==63378) \&\& (frame.time\_relative>30.0) \&\&( frame.time\_relative<50.0)}. The second, \textit{caida1s-pcap} is a trace with the first second from \texttt{wan-pcap}.


\subsection{Modelling Inter-Packet times and Packet Sizes: Proposed Process}

We summarize our process of modeling inter packet times at the figure~\ref{fig:model-parameterization}. We collect a set of inter-packet times from a actual traffic capture. Then, we estimate a set of parameters for stochastic functions, using different methodologies. Then, from these parametrized models, we estimate which best represent our data set, using the measure of quality AIC (Akaike information criterion). We also calculate another measure of quality called BIC(Bayesian information criterion), for comparison of results. In this chapter, we present our results obtained on our prototype implemented  in Octave.

Currently we are modeling:

\begin{itemize}
\item Weibull distribution, using linear regression, through the Gradient descendant algorithm;
\item Normal distribution, using direct estimation the mean and the standard deviation of the dataset;
\item Exponential distribution, using linear regression, through the Gradient descendant algorithm. We refer to this distribution as Exponential(LR);
\item Exponential distribution, using direct estimation of the dataset mean. We refer to this distribution as Exponential(Me);
\item Pareto distribution, using linear regression, through the Gradient descendant algorithm. We refer to this distribution as Pareto(LR);
\item Pareto distribution, using the maximum likelihood method. We refer to this distribution as Pareto(MLH);
\item Cauchy distribution, using linear regression, through the Gradient descendant algorithm;
\end{itemize}


%\textcolor{red}{TODO: Explciar quais dados são armazenados na base de dados e por que. Explicar como os dados são classificados, e modelados, e os parametros gerados. Citar métodos utilizado para parametrização, como calculo da média, maximmum likelihood e regressão linear. Apredentar o diagrama da ultima apresentação. Apresentar as equações utilizadas. Com esses dados bem como os plots dos dados linearizados, da corvergencia do custo e o fitting da CDF. Apresentar uma tabela com o valor do BIC e AIC para cada fitting. (Vai ficar muito grande a sessão)}




% In probability theory and statistics, the exponential distribution (a.k.a. negative exponential distribution) is the probability distribution that describes the time between events in a Poisson process, i.e. a process in which events occur continuously and independently at a constant average rate. It is a particular case of the gamma distribution. It is the continuous analogue of the geometric distribution, and it has the key property of being memoryless. In addition to being used for the analysis of Poisson processes, it is found in various other contexts.


Now we will give a brief explanation our three procedures: Linear Regression (with the  Gradient descendant algorithm), direct estimation, and maximum likelihood. We go deep in details on the explanation of the methods on the appendix ~\ref{ap:revision-probability}, and present the mathematical demonstrations on appendix~\ref{ap:mathematical-modeling}. 

Some observations must be made. Since the resolution of the time samples used were of $10^{-6}$s, all values equals to zero were set to  $5\cdot10^{-8}$s, to avoid division by zero. To avoid divergence on tangent operation on the linearization the Cauchy function, the inter packets CDF function values were floor-limited and upper-limited to  $10^{-6}$ and $0.999999$ respectively.

We implemented this prototype using Octave. Its code is located on GitHub, for reproduction purposes\footnote{github-link-aqui}


\subsubsection{Linear regression (Gradient descendant)}

Linear regression is a method for estimating the best linear curve

\begin{equation}
y = ax + b
\end{equation}

to fit a given data set. We can use linear regression to estimate parameters of a non-linear curve expressing it on a linear format. For example, the Weibull CDF for $t > 0$ is:

\begin{equation}
F(t; \alpha, \beta) = F(t) = 1 - e^{-(t/\beta)^{\alpha}}
\end{equation}

manipuling it, we can found:
\begin{equation}
\alpha\ln{(t)} - \alpha\ln{(\beta)} = \ln{-\ln{1 - F(t)}}
\end{equation}

%corrigir erro
If we call $x = \ln{(t)}$ and $y = \ln{(-\ln{(1 - F(t))})}$, we found a linear equation, where $a = \alpha$ and $b = -\alpha\ln{(\beta)}$. Having in hands a estimation of the empirical CDF of our data samples, we apply the $x$ and $y$ definitions to linearize the data. Using the gradient descendant, we are able to find a estimation of the linear coefficients $\hat{a}$ and $\hat{b}$. Using the inverse function of the definition of linear coefficients, we are able to find the estimated parameters $\hat{\alpha}$ and $\hat{\beta}$.

\begin{equation}
\alpha = a
\end{equation}

\begin{equation}
\beta = e^{-(b/a)}
\end{equation}

We present examples of linearized data, and the linear approximation achieved, as well the cost convergence of the gradient descendant algorithm in the appendix ~\ref{ap:mathematical-modeling}.


Applying the inverse equations of the linear coefficients ($\hat{\alpha} = \hat{a}$ and $\hat{\beta} = e^{-(\hat{b}/\hat{a})}$) (the symbol stands for the estimated parameters). So we are able to estimate the Weibull distribution parameters.

We can summarize this procedure, in these steps:
\begin{enumerate}
\item Linearize the stochastic CDF function F(t).
\item Apply the linearized $y = y(F(t))$ and  $x = x(t)$ on the empirical CDF and times datasets, respectively. 
\item Use Gradient Descendant algorithm to find linear coefficients $a$ and $b$.
\item Apply the inverse equation of the linear coefficients, to determine the stochastic function parameters.
\end{enumerate}

In the parameters estimation, there is an exception, since the Pareto scale ($t_{m}$) is defined by the minimum time. 

In the table~\ref{tab:linearization-sumary} we present a summary of the used equations in the procedure. In this notation, the subscript $i$ means that it must be applied on every empirical value measured. The hat( $\widehat{}$ ) indicates an estimated value for a parameter.

In the figure~\ref{fig:linearization} we presente the linearized 

\begin{table}[h!]
\centering
\caption{My caption}
\label{tab:linearization-sumary}
\begin{tabular}{llllll}
\hline
Function    & Linearized $x$     & Linearized $y$                    & \multicolumn{2}{l}{Parameters}                                                        &  \\
\hline
Cauchy      & $x_i = t_i$        & $y_i = \tan{(\pi(F(t_i) - 1/2))}$ & $\hat{\gamma} = \frac{1}{a}$ & $\hat{\hat{t_0}} = - \frac{b}{a}$                      &  \\
Exponential & $x_i = t_i$        & $y_i = \ln{(1 - F(t_i))})$        & \multicolumn{2}{l}{$\hat{\lambda} = -a$}                                              &  \\
Pareto      & $x_i = \ln{(t_i)}$ & $y_i = \ln{(1 = F(t_i))}$         & $\alpha = -a $         & $\hat{x_{m}} = \min_{i = 0, ..., m}\{x_{i}\}$ &  \\
Weibull     & $x = \ln{(t)}$     & $y = \ln{(-\ln{(1 - F(t))})}$     & $\alpha = a$                 & $\beta = e^{-(b/a)}$                                   & \\
\hline
\end{tabular}
\end{table}


\begin{figure}[ht!]
\centering
\subfloat[c]{
  \includegraphics[height=50mm]{figures/ch4/Skype_Weibull_-_Linearized_data_and_linear_fitting}
  \label{fig:linearization}
}
%\hspace{0mm}
\subfloat[c]{
  \includegraphics[height=50mm]{figures/ch4/Skype_Weibull_-_Cost_J_iterations__convergence}
  \label{fig:cost}
}
\label{fig:linearization-cost}
\end{figure}





\subsubsection{Direct Estimation}

%corrigir error
The expected value $E(X)$ and and variance $Var(X)$ of a random variable $X$ of some distributions are close related with its parameters. Since the mean $\bar{\mu}$ and the standard deviation $\bar{\sigma}$ are in general good approximations for the expected value and variance, we may estimate we may use them to estimate parameters.
%corrigir error
Following the notation presented at table~\ref{tab:distributions-equations}, we have for the normal distribution:
\begin{equation}
(\hat{\mu}, \hat{\sigma} = (\bar{\mu}, \bar{\sigma})
\end{equation}


For the exponential distribution $E(X) = \frac{1}{\lambda}$, therefore:
\begin{equation}
\hat{\lambda} = \frac{1}{\hat{\mu}}
\end{equation} 

\subsubsection{Maximum Likelihood}

The maximum likelihood estimation, is a method for estimation of parameters, winch maximizes the likelihood function. We explain in details this subject in the Appendix A. Using this method for the Pareto distribution, it is possible to derive the following equations the its parameters:
\begin{equation}
\hat{x_{m}} = \min_{i = 0, ..., m}\{x_{i}\}
\end{equation} 

\begin{equation}
\hat{\alpha} = \frac{n}{ \sum_{i = 0}^{m}(\ln{(x_{i}) - \ln(\hat{x_{m}})})  }
\end{equation} 

where $m$ is the sample size.


\subsubsection{AIC and BIC}

Suppose that we have an statistical model $M$ of soma dataset ${\boldsymbol{x} = (x_1, ..., x_n)}$, with $n$ independent and identically distributed observations of a random variable $X$. This model can be expressed by a PDF $f(x, \boldsymbol{\theta})$, where $\boldsymbol{\theta}$ a vector of parameter of the PDF, $\boldsymbol{\theta} \in \mathbb{R}^{k}$ ($k$ is the number of parameters). The  likelihood function  of this model $M$ is given by:

\begin{equation}
L(\boldsymbol{\theta}|\boldsymbol{x} ) =  f(x_1|\boldsymbol{\theta})...f(x_n|\boldsymbol{\theta}) = \prod_{i = 1}^{n}f(x_i|\boldsymbol{\theta})
\end{equation}

Now, suppose we are trying to estimate the best statistical model, from a set ${M_1, ..., M_n}$, each one whit an estimated vector of parameters  ${\boldsymbol{\hat{\theta_1}}}, ..., {\boldsymbol{\hat{\theta_n}}}$. $AIC$ and $BIC$ are defined by:

\begin{equation}
AIC = 2k - \ln(L(\boldsymbol{\hat{\theta}}|\boldsymbol{x}))
\end{equation}

\begin{equation}
BIC = k\ln(n) - \ln(L(\boldsymbol{\hat{\theta}}|\boldsymbol{x}))
\end{equation}

In both cases, the preferred model $M_i$, is the one with the smaller value of $AIC_i$ or $BIC_i$.




\subsubsection{Validation}

To see if our criterion of parameter selection is actually is able to find which is the best model, we define a validation methodology. 
We generate a vector with the same size form the original, with random generated data through our model estimation. Then we compare it with the original sample, trough four different metrics, all with a confidence interval of 95\%:

\begin{itemize}
\item Correlation between the sample data and the estimated model;
\item Hurst exponent;
\item Mean inter packet time;
\item Standard deviation of inter packet times.
\end{itemize}

The Pearson's product-moment coefficient, or simply correlation coefficient,  is an expression of the dependence or association between two datasets. Its value goes from -1 to +1. +1 means a perfect direct linear correlation. -1 indicates perfect inverse linear correlation. 0 means no linear correlation. So, as close the result reach 1, more similar are the inter packet times to the original values. To estimate it, we use the Octave's function \texttt{corr()}.


As explained before in the chapter~\ref{ch:literature-review}, the Hurst exponent is a measurer of self-similarity, and indicates the fractal level of the inter packet times. As close the result is from the original, more similar is the fractal level of the estimated samples from the original.To estimate this value we use the function \texttt{hurst()} from Octave, which uses rescaled range method.

Finally we measure the mean and the standard deviation (as a measure of the dispersion), using the Octave's functions \texttt{mean()} and \texttt{std()}. We also present some \textit{QQplots}, to visually compare the random-generated data and the actual dataset. 


As close the correlation, Hurst exponent, mean and the standard deviation is from the original dataset, the better is model fitting. Also, analyzing the mean, we can see if a certain modeling procedure tend to be more penalized for the values close, or far from zero. That means, if the mean inter-packet time tends to be smaller or higher compared to the original. With these results in hands, we can see if AIC and BIC are actually good criterion on model selection for inter-packet times.

\section{Results}

%%%%%%%%%%%%%%%%%%%%%%%
% Prototype results
\begin{table}[h!]
\centering
\caption{Results of the octave prototype, include BIC and AIC values, para estimated parameters for two of our pcap traces: \textit{skype-pcap }and \textit{bigflows-pcap}. \textit{bigflows-pcap} is much larger, and has a much much smaller mean inter-packet time}
\label{tab:prototype-results1}
\scalebox{0.70}{ 
\begin{tabular}{lllllllll}
\hline
                & \multicolumn{8}{c}{Trace} \\ \cline{2-9} 
Function		& AIC        & BIC        & \multicolumn{2}{c}{Parameters}              & AIC        & BIC        & \multicolumn{2}{c}{Parameters}                \\ \cline{2-9}
        & \multicolumn{4}{c}{skype-pcap}                                        & \multicolumn{4}{c}{bigflows-pcap}                                       \\ \hline 
Cauchy          & $ 21.8\cdot10^3 $  & $21.9\cdot10^3$  & $\gamma:0.000259 $ & $x_0:0.104891 $    & $ 7.2\cdot10^6 $   & $7.2\cdot10^6$   & $ \gamma:1.935833 $ & $x_0:-7.245697 $    \\
Exponential(LR) & $-4.3\cdot10^3$  & $ -4.3\cdot10^3 $  & \multicolumn{2}{l}{$\lambda:1.861121 $}   & $7.3\cdot10^6$   & $7.3\cdot10^6$   & \multicolumn{2}{l}{$ \lambda:0.009752 $}    \\
Exponential(Me) & $-1.6\cdot10^3$  & $-1.6\cdot10^3$  & \multicolumn{2}{l}{$\lambda:7.011624 $}   & $-10.9\cdot10^6$ & $-10.9\cdot10^6$ & \multicolumn{2}{l}{$ \lambda:2638.706529 $} \\
Normal          & $3.3\cdot10^3$   & $3.3\cdot10^3$   & $\mu:0.142620 $    & $\sigma:0.500698 $ & $-9.4\cdot10^6$  & $-9.4\cdot10^6$  & $ \mu:0.000379$     & $ \sigma:0.000660 $ \\
Pareto(LR)      & $-8.3\cdot10^3$  & $-8.3\cdot10^3$  & $\alpha:0.252367 $ & $x_m:0.000000 $    & $-10.3\cdot10^6$  & $-10.3\cdot10^6$  & $ \alpha:0.148862 $ & $ x_m:0.000000 $    \\
Pareto(MLH)     & $-11.6\cdot10^3$ & $-11.6\cdot10^3$ & $\alpha:0.092060 $ & $x_m:0.000000 $    & $-10.3\cdot10^6$  & $-10.3\cdot10^6$  & $ \alpha:0.136193 $ & $ x_m:0.000000 $    \\
Weibull         & $-14.6\cdot10^3$ & $-14.6\cdot10^3$ & $\alpha:0.319808 $ & $\beta:0.015244 $  & $-11.0\cdot10^6$ & $-11.0\cdot10^6$ & $ \alpha:0.281189 $ & $ \beta:0.000554 $  \\ \hline

				& \multicolumn{4}{c}{skype-pcap}                                        & \multicolumn{4}{l}{bigflows-pcap}                                       \\ \hline
Cauchy          & $ 21.8\cdot10^3 $  & $21.9\cdot10^3$  & $\gamma:0.000259 $ & $x_0:0.104891 $    & $ 7.2\cdot10^6 $   & $7.2\cdot10^6$   & $ \gamma:1.935833 $ & $x_0:-7.245697 $    \\
Exponential(LR) & $-4.3\cdot10^3$  & $ -4.3\cdot10^3 $  & \multicolumn{2}{l}{$\lambda:1.861121 $}   & $7.3\cdot10^6$   & $7.3\cdot10^6$   & \multicolumn{2}{l}{$ \lambda:0.009752 $}    \\
Exponential(Me) & $-1.6\cdot10^3$  & $-1.6\cdot10^3$  & \multicolumn{2}{l}{$\lambda:7.011624 $}   & $-10.9\cdot10^6$ & $-10.9\cdot10^6$ & \multicolumn{2}{l}{$ \lambda:2638.706529 $} \\
Normal          & $3.3\cdot10^3$   & $3.3\cdot10^3$   & $\mu:0.142620 $    & $\sigma:0.500698 $ & $-9.4\cdot10^6$  & $-9.4\cdot10^6$  & $ \mu:0.000379$     & $ \sigma:0.000660 $ \\
Pareto(LR)      & $-8.3\cdot10^3$  & $-8.3\cdot10^3$  & $\alpha:0.252367 $ & $x_m:0.000000 $    & $-10.3\cdot10^6$  & $-10.3\cdot10^6$  & $ \alpha:0.148862 $ & $ x_m:0.000000 $    \\
Pareto(MLH)     & $-11.6\cdot10^3$ & $-11.6\cdot10^3$ & $\alpha:0.092060 $ & $x_m:0.000000 $    & $-10.3\cdot10^6$  & $-10.3\cdot10^6$  & $ \alpha:0.136193 $ & $ x_m:0.000000 $    \\
Weibull         & $-14.6\cdot10^3$ & $-14.6\cdot10^3$ & $\alpha:0.319808 $ & $\beta:0.015244 $  & $-11.0\cdot10^6$ & $-11.0\cdot10^6$ & $ \alpha:0.281189 $ & $ \beta:0.000554 $  \\ \hline

\end{tabular}
}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%
% Logscale Skype

\begin{figure}[ht!]
\centering
\label{fig:aproximation-original-cdf}
\subfloat[Chauchy]{
  \includegraphics[width=78mm]{figures/ch4/Skype_Logscale_-_Cauchy_aproximation_vs_Original_set}
}
\subfloat[Exponential(LR)]{
  \includegraphics[width=78mm]{figures/ch4/Skype_Logscale_-_Exponential_aproximation__linear_regression__vs_Original_set}
}
\hspace{0mm}
\subfloat[Exponential(Me)]{
  \includegraphics[width=78mm]{figures/ch4/Skype_Logscale_-_Exponential_aproximation__mean__vs_Original_set}
}
\subfloat[Normal]{
  \includegraphics[width=78mm]{figures/ch4/Skype_Logscale_-_Normal_aproximation_vs_Original_set}
}
\hspace{0mm}
\subfloat[Pareto(LR)]{
  \includegraphics[width=78mm]{figures/ch4/Skype_Logscale_-_Pareto_aproximation__linear_regression__vs_Original_set}
}
\subfloat[Pareto(MLH)]{
  \includegraphics[width=78mm]{figures/ch4/Skype_Logscale_-_Pareto_aproximation__maximum_likehood__vs_Original_set}
}
\hspace{0mm}
\subfloat[Weibull]{
  \includegraphics[width=78mm]{figures/ch4/Skype_Logscale_-_Weibull_aproximation_vs_Original_set}
}
\caption{CDF functions for the approximations of \textit{skype-pcap} inter  packet times, of many stochastic functions.}
\end{figure}


%%%%%%%%%%%%%%
% Skype correlation
\begin{figure}[h!]
\centering
\label{fig:correlation-hurst-skype-pcap}
\subfloat[Correlation]{
  \includegraphics[width=75mm]{figures/ch4/Skype_Correlation}
  \label{correlation-skype-pcap}
}
\hspace{0mm}
\subfloat[Hust Exponent]{
  \includegraphics[width=75mm]{figures/ch4/Skype_Hurst_Exponent}
}
\hspace{0mm}
\subfloat[Mean]{
  \includegraphics[width=75mm]{figures/ch4/Skype_Mean}
}
\hspace{0mm}
\subfloat[Standard Deviation]{
  \includegraphics[width=75mm]{figures/ch4/Skype_Standard_Deviation}
  \label{hurst-skype-pcap}
}
\caption{Statistical parameters of \textit{skype-pcap} and its approximations}
\end{figure}

%%%%%%%%%%%%%%
% BigFlows correlation
\begin{figure}[ht!]
\centering
\label{fig:correlation-hurst-big-flows-pcap}
\subfloat[]{
  \includegraphics[width=75mm]{figures/ch4/bigFlows_Correlation}
  \label{correlation-skype-pcap}
}
\hspace{0mm}
\subfloat[]{
  \includegraphics[width=75mm]{figures/ch4/bigFlows_Hurst_Exponent}
}
\hspace{0mm}
\subfloat[]{
  \includegraphics[width=75mm]{figures/ch4/bigFlows_Mean}
}
\hspace{0mm}
\subfloat[]{
  \includegraphics[width=75mm]{figures/ch4/bigFlows_Standard_Deviation}
  \label{hurst-skype-pcap}
}
\caption{Statistical parameters of \textit{bigflows-pcap} and its approximations}
\end{figure}


% BigFlows correlation
\begin{figure}[ht!]
\centering
\label{fig:correlation-hurst-big-flows-pcap}
\subfloat[]{
  \includegraphics[width=75mm]{figures/ch4/Skype_QQplot_-_Weibull_dat}
  \label{correlation-skype-pcap}
}
\hspace{0mm}
\subfloat[]{
  \includegraphics[width=75mm]{figures/ch4/Skype_QQplot_-_Cauchy}
}
\caption{Statistical parameters of \textit{bigflows-pcap} and its approximations}
\end{figure}

The results for the parameters estimation and AIC and BIC for \textit{skype-pcap} and \textit{bigflows-pcap} are at the table~\ref{tab:prototype-results1}.  The results for \textit{wan-pcap} and \textit{lan-diurnal-pcap} are on the table ~\ref{tab:prototype-results2}. The difference between the values of BIC and AIC in all simulations were very small. Much smaller then the difference between the values found for each distribution. This is an indication that for our type of dataset, using AIC or BIC should will not influence significantly the results. 

On our previsions, Weibull followed Pareto are the functions more indicated as good options. This was expected, since both are heavy-tailed functions. But, Cauchy, on most of the tests, seems to no present a good fitting. This is effect of the fast divergence of tangent function, when we linearize our data. Not letting the CDF values be so close to zero or avoided divergence, but still through linear regression the penalty was heavy. 


In the figure ~\ref{fig:aproximation-original-cdf} we present all estimated CDF functions along with the empirical CDF, for the trace \textit{skype-pcap}. In the case of the normal function, all values smaller than zero, were set to zero on the plot. Is possible to see different accuracies on each plot, and types of fittings. Visually, the best fit seems to be the Weibull trough linear regression. In the figure ~\ref{fig:aproximation-originalbigflows-cdf} we present the fittings with higher BIC and AIC values for the \textit{bigflows-pcap}. On our previsions. Analyzing the plots, and what they would mean, our Cauchy fitting would impose an almost contant traffic , with the inter packet time close to the mean. On the trace \textit{skype-pcap} the exponential plots seems to not represent well the small values of inter packet times. On the other hand, on \textit{bigflows-pcap} they are expressed much well. This is due fact that an exponential process is good at representing values close to the mean. But, it fails to represent values too small and too higher. The \textit{bigflows-pcap}  has a much smaller dispersion. On the other hand, a self-similar process like Weibull and Pareto are better representing inter packet times with higher dispersion. 

Analyzing the quality of AIC and BIC as criterion of choose on \textit{skype-pcap}, we see that in therms of Correlation and Self-similarity it picket the right model: Weibull. Also in therms of mean packet rate and dispersion, is still a good choice, since it is still one of the best choices. The third and the fourth choices (Pareto(LR)) and Exponential(LR) also are good options in most of these metrics. But, Pareto(MLH) is presented as the second best choice, and it had poor results in comparison to the others, especially on mean, correlation and dispersion. In this example, we illustrate it at the table ~\ref{tab:skype-evaluation}.

The trace \textit{bigflows-pcap} has a much higher inter packet rate, and a smaller dispersion. Weibull is the model chosen by AIC/BIC, and it is the second best on correlation and self-similarity (Exponential(Me) and Cauchy are the best respectively). Exponential(Me) chooses as the second preferable, has the closest to the original mean and standard deviation. Comparing the plots of the CDF function, the Weibull seams to express well both small and larger values of inter packet times, while Exponential(Me) is preciser on the mean values, as we can see at figure ~\ref{fig:aproximation-originalbigflows-cdf}. 

%%%%%%%%%%%%%%%%%%%%%%%
% Logscale bigFlows
\begin{figure}[ht!]
\centering
\subfloat[]{
  \includegraphics[width=80mm]{figures/ch4/bigFlows_Logscale_-_Exponential_aproximation__mean__vs_Original_set}
  \label{correlation-skype-pcap}
}
%\hspace{0mm}
\subfloat[]{
  \includegraphics[width=80mm]{figures/ch4/bigFlows_Logscale_-_Weibull_aproximation_vs_Original_set}
}
\label{fig:aproximation-originalbigflows-cdf}
\end{figure}


On \textit{lan-firewall-pcap}

On \textit{wan-pcap}


%%%%%%%%%%%%%%%%%%%%%%%
% Prototype results
%%%%%%%%%%%%%%
% BigFlows correlation
\begin{figure}[ht!]
\centering
\label{fig:correlation-hurst-big-flows-pcap}
\subfloat[]{
  \includegraphics[width=75mm]{figures/ch4/Skype_costFunction}
  \label{correlation-skype-pcap}
}
\hspace{0mm}
\subfloat[]{
  \includegraphics[width=75mm]{figures/ch4/bigFlows_costFunction}
}
\hspace{0mm}
\subfloat[]{
  \includegraphics[width=75mm]{figures/ch4/Lan_costFunction}
}
\hspace{0mm}
\subfloat[]{
  \includegraphics[width=75mm]{figures/ch4/Wan_costFunction}
  \label{hurst-skype-pcap}
}
\caption{Statistical parameters of \textit{bigflows-pcap} and its approximations}
\end{figure}





%%%%%%%%%%%








%\begin{figure}[ht!]
%\centering
%\subfloat[]{
%  \includegraphics[width=80mm]{figures4/Skype_QQplot_-_Cauchy}
%  \label{correlation-skype-pcap}
%}
%\hspace{0mm}
%\subfloat[]{
%  \includegraphics[width=80mm]{figures4/Skype_QQplot_-_Weibull_dat}
%}
%\label{correlation-hurst-skype-pcap}
%\end{figure}

\section{Others Algorithms}

\subsection{On/Off Times estimation}

\begin{algorithm}
\caption{calcOnOff}
\begin{algorithmic}[1]
	\Function{calcOnOff}{$arrivalTime, deltaTime, cutTime, minOnTime$}%\Comment{Where A - array, p - left, q - middle, r - right}
    \State $m = deltaTime.length() - 1$
    \State $j = 0$
    \State $lastOff = 0$
    \State $pktCounterSum = 0$
    \State $fileSizeSum = 0$
	\For{$i = 0:m$}
    	\State $pktCounterSum = pktCounterSum + 1$
		\State $fileSizeSum = fileSizeSum + psSizeList[i, 1]$
    	\If{$i == 1$} \Comment{if the first is session-off time}
        	\State $j++$
            \State $onTimes.push(minOnTime)$
            \State $offTimes.push(deltaTime[i])$
            	\State $pktCounter.push(pktCounterSum)$
				\State $fileSize.push(fileSizeSum)$
				\State $pktCounterSum = 0$
				\State $fileSizeSum = 0$
            \Else \Comment{base case} 
            	\State $pktCounter.push(pktCounterSum)$
				\State $fileSize.push(fileSizeSum)$
				\State $pktCounterSum = 0$
				\State $fileSizeSum = 0$
            	\If{$j == 0$}
                	\State $onTimes.push(arrivalTime[i - 1])$
					\State $offTimes.push(deltaTime[i])$
                \Else \Comment{others on times} 
					\State  $onTimes.push(max(deltaTime[i-1] - deltaTime[lastOff], minOnTime))$ 
					\State  $offTimes.push(deltaTime[i])$
                \EndIf
                \State $lastOff = i$
            \EndIf        
    \EndFor
    \State $pktCounterSum = pktCounterSum + 1$
	\State $fileSizeSum = fileSizeSum + psSizeList[m]$
    \If{$lastOff == m - 1$} \Comment{ if last is session-off }
		\State $onTimes.push(minOnTime)$ % on time
	\Else \Comment{ base last case}
		\If{$lastOff \neq 0$}
		\State $onTimes.push(arrivalTime[m] - arrivalTime[lastOff])$ 
		\Else 
		\State $onTimes.push(arrivalTime[m])$ \Comment{there was just on time}
		\EndIf
	\EndIf
    \State $pktCounter.push(pktCounterSum)$
	\State $fileSize.push(fileSizeSum)$
    \EndFunction
\end{algorithmic}
\end{algorithm}


\subsection{Application classification}

\begin{table}[]
\centering
\caption{Application classification table}
\label{my-label}
\begin{tabular}{lll}
\hline
Application Protocol & Transport Protocols & Transport Ports \\ \hline
HTTPS               & TCP                & 443             \\
FTP                 & TCP                & 20, 21          \\
HTTP                & TCP                & 80              \\
BGP                 & TCP                & 179             \\
DHCP                & UDP                & 67, 68          \\
SNMP                & UDP, TCP           & 161             \\
DNS                 & UDP, TCP           & 53              \\
SSH                 & UDP, TCP           & 22              \\
Telnet              & UDP, TCP           & 23              \\
TACACS              & UDP, TCP           & 49              \\ \hline
\end{tabular}
\end{table}




\section{Implementation Details}

\subsection{Modeling Process}

\subsection{Matlab prototyping and C++ Implementation}

\subsection{Software Engineering process and Lessons Leaned}

\section{Conclusion}

We can conclude by this evaluation, that our chosen methodology for selecting stochastic models for inter packet data is appropriated. The first thing to observe is that, for a high 

First, we do not rely on only one type of parametrization. This turns our methodology more robust, since a linear regression may diverge. But aways there will be an packable model, since our estimations for Normal, Exponential(Me) and Pareto(MLH) cannot. Models wich the linear regression diverge, will not have god BIC/AIC estimations, and will not be picked. 



% https://en.wikipedia.org/wiki/Gamma_process
%Intordução
%- Como previamente discutido, há uma farta bibliografia dedicada ao estudo da natureza do trafego de internet. Em especial, relacionada a modelos que possam expressar a natureza do tempos entre pacotes. 
%- Como discutido anteriormente, Sabemos pela literatura que o trafego de internet possui uma caracteristica fractal. Processos tipicos de Poisson (expresso em sua forma contínua por uma funções estocastica esponencial)  não são capazes de representar de maneira realistica o trafego. Para tal, podem ser utilizados processos estocasticos heavy-tailed.
%- Porém não são numerosos os trabalhos que tratam da parametrização desses processos. Qual o melhor tipo de parametirzação é uma questão difícil de ser respondida, por diversas. Primeiramente, utilizar funções heavy-tailed pode ser uma metodologia mais eficaz para se garantir a self-similarity do trafego. Porém não necessáriamente outras importantes características do trafego serão mantidas, como por exemplo: média de pacotes por segundo, disperssão e correlação entre o trafego real e o trafego sintético gerado por esse processo. Não há a prncípio uma carantia que a escolha baseada em self-similatrity também irá garantir que essas outras características se mantanham. O ideal é que  encontremos um método que possa garantir um bom resultado na maioria, se não em todas essas características.
%- Além disso, há diversos métodos disponíveis para se estimar parâmertos de uma função estocastica. Para citar alguns exemplos:
%* Em alguamas funções como a normal e a exponencial os parâmetros podem ser estimados diratemente através do calculo da média e da variância dos dados.
%* Em funções que é possivel realizar a linearização, os parametros podem ser estimados por meio de regressão linear
%* Outro método frequentemente usado é o do maximum likelihood
%Porém, prever comportamento que cada funções estocástica terá com cada um desses estimadores, não é trivial. Por exemplo.  Por exemplo:
%* Dependendo dos dados originais, a estimativa pode ser muito pobre, e posuir uma baixa correlação com os dados originais. 
%* Os estimadores possuirão algum "bias", desviando apresentando um desvio para cima ou para baixo no valor esperado. Ou isso pode não ocorrer.
%- O objetivo desse capítulo é somente avaliar a qualidade de nosso modelo para estimativa de parâmetros estocasticos para estimadores de inter packet times. iremos utilizar o nosso procedimento em diferentes datasets, e avaliar a qualidade dos resultados obtidos.

\begin{comment}

%%%%%%%%%%%%%%

%To evaluate the quality of the generated data, we use four criteria: 

%As we can see in the figure~\ref{correlation-skype-pcap}, Weibull has a correlation close to one correlation (0.969). The follow values of next values of correlation are: Pareto(LR)(0.810), Normal (0.701), Exponential(LR) (0.697), Pareto(MLH) (0.556), Cauchy (0.470), and Exponential(Me) (0.294).

%The next criterion is The Hurst exponent. First, we can observe that all fittings have resulted in a  self-similar process since all values were between 0.5 and 1. Again, the AIC prediction was confirmed, since the Weibull fitting is the one with the closest fractal level from the original trace, around 0.80 and 0.85.

Revisão da literatura
* O trabalho do sourcesonoff realisa uma análise extensiva de uma modelagem apropriada entre bursts de trafego. No trabalho, algumas funções estocasticas são analizadas, e é recomendado o uso sa Weibull. Nesse trabalho é utilizado o calculo do prâmetro BIC como estimador da melhor função estocastica


Metodologia e validação
- por questões referentes a teoria da iinformação, optamos por utilizar o parametro AIC como nosso estimador oficial de melhor função estocastica. 
- Porém, também calculamos o BIC, e verificamos ue via de regra a diferença dos dois valores é bem baixa. Em geral, para datasets maiores, a diferença entre o BIC e AIC de uma mesma função tende a ser consideravelmente menor do que se comparado ao de outra funçao, como veremos a seguir
- A metodologia escolhida é a seguinte: realizamos a estimativa de parametors de diversas funções estocasticas para um mesmo dataset. Calcularemos o valor do AIC, e baseado nele, iremos estimar, em ordem crescente qual a melhor função estocástica. 
- fazer uma figura com cores, mostrando segundo o estimador a ordem das funções, e compara-las com  a ordem estimada por meio dos 4 outros parâmetros.
 - Utiizar correlação, media, variancia, e hurst.
- A vantagem do AIC é que tendo a disposição o datset original, ele é um metodo completamente analítico, não dependendo da geração de numeros aleatórios (como por exemplo no cauculo da correação). Nesse caso, evita que o gerador escolhido possua algum bias na geração dos dados, e comprometa o resultado do estimaor. Além disso, é um procedimento computacionalmente mais barato, pois é puramente analitico

Resultados
- Neste capítulo apresentaremos a análise de somente  2 datasets: da captura de skype e da captura big-flows. No apendice haverão outras análises, que incluem:
* uma análise de inter packets times de uma captura diurnal
* inter-packets times de um single flow do caida
* inter-burst timesconsiderando apenas inter-packet times maiores do que 1 segundo.
* inter pacekt time de 1 segundo do trace do caida.
- o porposito desse modelo é avaliar a qualidade do modelo de seleção, se ele é uma boa ou mpa escolha, já que não há muitso trabalhos relacionados na literatura.
- como o trafego de internet é fractal, caso essa metodologia represente uma boa escolha para os datsets analisados, ele também tende a ser uma boa escolha para datasets de diferentes intervalos de inter-packet times (maiores e menores). Uma análise mais precisa é feita no apendice a respeito dessa questão.






Conclusões







%In this chapter we will go deep into details on how our tool makes the data modeling, focusing on inter-packet times. We will present curves obtained by our Matlab prototype that shows in detail how each step works. After explaining this process, we will present an evaluation method of our results, using \textit{QQplots}. After this, we give some details of how we convert code from Matlab to C++ and its benefits on time execution. All code used in this section is available on our GitHub page, on the directory \texttt{Prototypes}. 

%We also define here the datasets we are going to use in the rest of this text. We will use two datasets, and for reproduction purposes, one is public available. The first is a CAIDA\footnote{http://www.caida.org/home/}{http://www.caida.org/home/}, and can be found at  \href{https://data.caida.org/datasets/passive-2016/equinix-chicago/20160121-130000.UTC}{https://data.caida.org/datasets/passive-2016/equinix-chicago/20160121-130000.UTC}. Access to this file need login, so you will have to create an account and wait for approval first. The pcap's file name is \texttt{equinix-chicago.dirA.20160121-130500.UTC.anon.pcap.gz}. The second we capture in our laboratory LAN, through a period of 24 hours. Along with other tests, We intend to verify diurnal behavior on it. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%After we implement this models, we where able to get different approximations for the stochastic functions. To evaluate how well the method is able to fit inter packet times, we first test it against a whole set of inter packet times. Figures (...) represents the CDF fitting for the function for the \textit{skype-pcap}. In the apendix ~\ref{ap:mathematical-modeling}, we also present the fitting achieved for whole set of inter-packet times, and for a single selected flow of \textit{caida-pcap} and\textit{lan-diurnal-pcap} \textit{pcaps} . We are able to see that the Weibull approximation fits better then any exponential approximation. But, we define an automatic way to estimate the best fitting. We  use the AIC (Akaike information criterion), winch is gave by:

%\begin{equation}
%AIC = 2k - 2\ln{(L)} 
%\end{equation} 

%where $k$ is the number os estimated parameters for the stochastic function, and $L$ is the value of the likelihood function. We explain it in details in the appendix A. The smaller is the AIC value, the best is the function fitting. An alternative for the AIC, is the BIC value, which follow the same rule. Big is defined by the equation:

%\begin{equation}
%BIC = k\ln{(n)} - 2\ln{(L)}
%\end{equation}

%where n is the number of elements of the sample dataset. We present at table ~\ref{tab:bic-aic} the values found by each estimator, for the \textit{skype-pcap}.

%As we can see, both BIC and AIC agree on each element, which one is the best and the worst. In this case, the best is the Weibull and the worst is the Cauchy. The exponential models, which are the continuous version of a classical Poisson process, is worse than Weibull and both Pareto estimation.

%But, to verify if our estimation is good, we need to see what sort of data these stochastic process are able to generate, and how close the stochastic properties of the synthetic dataset is from the original. In this way, we will be able to verify its actual quality in  unbiased way.



% The Pearson correlation is +1 in the case of a perfect direct (increasing) linear relationship (correlation), −1 in the case of a perfect decreasing (inverse) linear relationshi



%\textcolor{red}{TODO:Fazer uma breve avaliação das parametrizações obtidas com os dois traces utilizando QQplots, e comentar os resultados }


%\section{C codification}

%\textcolor{red}{TODO: Explicar detalhes da codificação em C, como bibliotecas utilizadas, a utilização de valores minimos para evitar o calculo de $\log{0}$, e a opção de executar testes de regressão, compilando com a opção regression-tests ou outra que eu criar na hora.}
\end{comment}